{
    "from": "google",
    "scholar_id": "1KuTc01GEJsJ",
    "detail_id": null,
    "title": "Towards open-world recommendation with knowledge augmentation from large language models",
    "abstract": "Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs \u2014 the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei\u2019s news and music recommendation platforms and gain a 7% and 1.7% improvement in the online A/B test, respectively.",
    "bib_name": "xi2024towards",
    "md_text": "# Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/115b/115baa3e-82ce-48e1-9f27-b040bbc0cad5.png\" style=\"width: 50%;\"></div>\nRui Zhang rayteam@yeah.net ruizhang.info Shenzhen, China\n# ABSTRACT\nRecommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs \u2014 the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei\u2019s news and music recommendation platforms and gain a 7% and 1.7% improvement in the online A/B test, respectively.\narXiv:2306.10933v4\n# 1 INTRODUCTION\nRecommender systems (RSs) are ubiquitous in today\u2019s online services, shaping and enhancing user experiences in various domains\n\u2217Both authors contributed equally to this research.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef95/ef954100-87d3-4aed-aabf-bb022cd681e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Comparison between (a) closed recommender systems and (b) open-world recommender systems.</div>\nsuch as movie discovery [32], online shopping [20], and music streaming [61]. However, a common characteristic of existing recommender systems is their insulated nature \u2014 the models are trained and deployed within closed systems. As depicted in Figure 1(a), the data utilized in a classical recommender system is confined to one or a few specific application domains [32, 79], isolated from the knowledge of the external world, thereby restricting the information that could be learned for a recommendation model. In fact, knowledge beyond the given domains can significantly enhance the predictive accuracy and the generalization ability of recommender systems [12, 38]. Hence, in this work, we posit that instead of solely learning from narrowly defined data in the closed systems, recommender systems should be the open-world systems that can proactively acquire knowledge from the external world, as shown in Figure 1(b). In particular, two types of information from the external world are especially useful for recommendation, which we refer to as open-world knowledge for recommendation \u2014 the reasoning knowledge on in-depth user preferences which is inferred from user behaviors and profiles, and the factual knowledge on items that can\nbe directly obtained from the web. On the one hand, the reasoning knowledge inferred from the user behavior history enables a more comprehensive understanding of the users, and is critical for better recommendation performance. Deducing the underlying preferences and motives that drive user behaviors can help us gain deeper insights and clues about the users. A person\u2019s personality, occupation, intentions, preferences, and tastes could be reflected in their behavior history. This preference reasoning can even integrate seasonal factors (e.g., holiday-themed movie preferences during Christmas) or external events (e.g., an increased interest in health products during a pandemic) and provide human-like recommendations with clear evidence, which goes beyond identifying basic behavior patterns as in classical recommenders. On the other hand, the factual knowledge on items provides valuable common sense information about the candidate items and thereby improves the recommendation quality. Take movie recommendation as an example, the external world contains additional movie features such as plots, related reports, awards, critic reviews that have not been included in the recommendation dataset, which expands the original data and is beneficial to the recommendation task. Several existing studies attempt to complement the closed recommender systems with additional information by knowledge graphs [17, 63] or multi-domain learning [18, 58]. However, constructing comprehensive and accurate knowledge graphs or multidomain datasets requires considerable extra human effort, and the accessible knowledge remains limited. Moreover, they only focus on extracting the factual knowledge from the external world, and overlook the reasoning knowledge on user preferences [17]. For instance, knowledge graphs used in RS usually focus on complement knowledge for items and rarely consider users [17, 62, 63] since the user-side knowledge is highly dynamic and challenging to capture within a fixed knowledge graph. Recent rapid developments in large language models (LLMs) have revolutionized the learning paradigm of various research fields and show great potential in bridging the gap between classical recommenders and open-world knowledge [76]. With the immense scale of the model and the corpus size, these large pretrained language models like GPT-4 [47], LLaMA [60] have shown remarkable capabilities, such as problem solving, logical reasoning, creative writing [4]. Learning from an extensive corpus of internet texts, LLMs have encoded a vast array of world knowledge \u2014 from basic factual information to complex societal norms and logical structures [4]. As a result, LLMs can perform basic logical reasoning that aligns with known facts and relationships [67, 77]. Recently, a few studies have attempted to apply LLMs as recommenders by converting recommendation tasks and user profiles into prompts [7, 13, 41]. Though some preliminary findings have been obtained, the results of using LLMs as recommenders are far from optimal for real-world recommendation scenarios due to the following shortcomings. 1) Predictive accuracy. The accuracy of LLMs is generally outperformed by classical recommenders in most cases, since LLMs have not been trained on specific recommendation data [41]. The lack of recommendation domain knowledge and collaborative signals prevents LLMs from adapting to individual user preferences. 2) Inference latency. Due to the excessive number of model parameters, it is impractical to directly use LLMs as recommender systems in industrial settings. With billions of\nusers and thousands of user behaviors, LLMs fail to meet the low latency requirement in recommender systems (usually within 100 milliseconds). The large model size also hinders the possibility of employing real-time user feedback to update and refine the model as in classical recommenders. 3) Compositional gap. LLMs often suffer from the issue of compositional gap, where LLMs have difficulty in generating correct answers to the compositional problem like recommending items to users, whereas they can correctly answer all its sub-problems [52]. Requiring direct recommendation results from LLMs is currently beyond their capability and cannot fully exploit the open-world knowledge encoded in LLMs [7, 30]. Therefore, the goal of this work is to effectively incorporate openworld knowledge while preserving the advantages of classical recommender systems. However, despite the appealing capabilities of LLMs, extracting and utilizing knowledge from them is a non-trivial task. For one thing, LLMs encode vast corpora of world knowledge across various scenarios. Identifying useful external knowledge for recommendation and eliciting the accurate reasoning process on user preferences are quite challenging. Moreover, the world knowledge generated by LLMs is in the form of human-like texts and cannot be interpreted by recommendation algorithms. Even if some LLMs are open-sourced, the decoded outputs are usually large dense vectors (e.g., 4096 for each token), which are highly abstract and not directly compatible with recommender systems. Effectively transforming the output knowledge to be compatible with the recommendation space, without information loss or misinterpretation, is pivotal for the quality of the recommendation. Besides, the knowledge generated by LLMs can sometimes be unreliable or misleading due to the hallucination problem [29]. Hence, it is critical to increase the reliability and availability of the generated knowledge to fully unleash the potential of open-world recommender systems. To address the above problems, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, (dubbed KAR). KAR is a model-agnostic framework that bridges classical recommender systems and open-world knowledge, leveraging both reasoning and factual knowledge from the LLMs. By first leveraging LLMs to generate open-world knowledge, and then applying classical recommender systems to model the collaborative signals, we combine the advantages of both LLMs and RSs and significantly improve the model\u2019s predictive accuracy. We also propose to prestore the obtained knowledge to avoid the inference latency issue when incorporating LLMs in RSs. Specifically, KAR consists of three stages: (1) knowledge reasoning and generation, (2) knowledge adaptation, and (3) knowledge utilization. For knowledge reasoning and generation, to avoid the compositional gap, we propose factorization prompting to break down the complex preference reasoning problem into several key factors to generate the reasoning knowledge on users and the factual knowledge on items. Then, the knowledge adaptation stage transforms the generated knowledge to augmented vectors in recommendation space. In this stage, we propose hybrid-expert adaptor module to reduce dimensionality and ensemble multiple experts for robust knowledge learning, thus increasing the reliability and availability of the generated knowledge. Finally, in the knowledge utilization stage, the recommendation model incorporates the augmented vectors with original domain features for prediction, combining\nboth the recommendation domain knowledge and the open-world knowledge. Our main contributions can be summarized as follows: \u2022 We present an open-world recommender system, KAR, which bridges the gap between the recommendation domain knowledge and the open-world knowledge from LLMs. To the best of our knowledge, this is the first practical solution that introduces logical reasoning with LLMs for user preferences to the recommendation domain. \u2022 KAR transforms the open-world knowledge to dense vectors in recommendation space, which are compatible with any recommendation models. We also release the code of KAR and the generated textual knowledge from LLMs1 to facilitate future research. \u2022 The knowledge augmentation can be preprocessed and prestored for fast training and inference, avoiding the large inference latency when adopting LLMs to RSs. Now, KAR has been deployed to Huawei\u2019s news and music recommendation platforms and gained a 7% and 1.7% improvement in the online A/B test. This is one of the first successful attempts in deploying LLM-based recommender to real-world applications. Extensive experiments conducted on public datasets show that KAR significantly outperforms the state-of-the-art models, and is compatible with various recommendation algorithms. We believe that KAR not only sheds light on a way to inject the knowledge from LLMs into the recommendation models, but also provides a practical framework for open-world recommender systems in large-scale applications.\n# 2 RELATED WORK\nThis section reviews studies on recent advances in recommendation with pretrained language models (PLMs).\n# 2.1 PLM as Recommender Itself\nThe emergence of pretrained language models (PLMs) has brought tremendous success in Natural Language Processing (NLP), and PLMs also show great potential in other domains like recommendations [69]. One promising direction is to leverage PLMs as the primary driver of recommendations, allowing PLMs to directly accomplish the recommendation tasks [35, 69, 71]. For example, LMRecSys [75] is one of the earliest attempts to transfer the sessionbased recommendation task into prompts and evaluate the performance of BERT [31] and GPT-2 [55] in movie recommendation. Later, P5 [14] and M6-Rec [6] finetune pretrained language models (T5 [56] or M6 [40]) by converting multiple recommendation tasks to natural language sequences to incorporate knowledge and semantics inside the training corpora for personalization. Similarly, RecFormer [34] models user preferences and item features as language representations for sequential recommendation. In this earlier stage, the sizes of the language models for recommendation are relatively small (e.g., under billions of parameters), and finetuning is usually involved for better performance. With the scaling of the model size and corpus volume, especially with the emergence of ChatGPT [47], LLMs have shown uncanny\n1Code and knowledge are available at https://gitee.com/mindspore/models/tree/ master/research/recommend/KAR and https://github.com/YunjiaXi/Open-WorldKnowledge-Augmented-Recommendation\ncapability in a wide variety of tasks [11]. One of the unique abilities of LLMs is reasoning, which emerges only when the model size surpasses a certain threshold. Zero-shot learning or in-context learning is widely used since finetuning LLMs requires lots of resources. Several studies apply LLMs as recommenders and achieve some preliminary results [7, 13, 23, 30, 41, 64]. For instance, ChatRec [13] employs LLMs as a recommender system interface for conversational multi-round recommendations. Liu et al. [41] study whether ChatGPT can serve as a recommender with task-specific prompts and report the zero-shot performance. Hou et al. [23] further report the zero-shot ranking performance of LLMs with historical interaction data. However, directly using LLMs as recommenders generally falls behind state-of-the-art recommendation algorithms, implying the importance of domain knowledge and collaborative signals for recommendation tasks [7, 30, 39]. Therefore, there are also methods exploring the incorporation of recommendation collaborative signals into LLMs through parameter-efficient finetuning approaches [42]. For example, TALLRec [2] finetunes LLaMA-7B model [60] with a LoRA [24] architecture on recommendation data. Another study [19] finetunes an Open-AI ada model2 on recommendation data but finds its performance lag behind utilizing the embedding of LLM for similarity matching or as an initialization for recommendation model. Moreover, these preliminary studies mainly overlook the inference latency during deployment and the compositional gap problem of LLMs. In this work, we propose factorization prompting to extract both reasoning and factual knowledge from LLMs, alleviating the compositional gap. This knowledge is then adapted to the recommendation domain as augmented representation vectors, which can be prestored for fast training and inference.\n# 2.2 PLM as Component of Traditional Recommender\nUnlike the above approaches where PLMs are used as the primary driver of recommendations, another category of work leverages PLMs as an auxiliary component in traditional RSs. Here, PLMs are usually adopted to encode the textual features (e.g., item descriptions, user reviews) or provide extra knowledge for classical recommendations for better user or item representations [9, 21, 22, 54]. For example, U-BERT [54] leverages the embeddings of user review texts encoded by BERT [31] to complement user representations. ZEREC [9] incorporates traditional recommender systems with PLMs to generalize from a single training dataset to other unseen testing domains. UniSRec [22] utilizes BERT [31] to encode user behaviors, and therefore learns universal sequence representations for downstream recommendation. Built upon UniSRec, VQ-Rec [21] further adopts vector quantization techniques to map language embeddings into discrete codes, balancing the semantic knowledge and domain features. The above methods usually use small PLMs (e.g., BERT-base with 110M parameters) to convert texts to dense vectors, the semantic information of which could be limited and thus fail to provide strong assistance for traditional recommender systems.\n2https://platform.openai.com/docs/guides/fine-tuning\nAnother line of work adopts large language models with billionlevel parameters, focusing on encoding or prompting external openworld knowledge from LLMs. For instance, some researchers propose S&R Multi-Domain Foundation model [15], which finetunes ChatGLM2-6B [10] to extract domain invariant features for promoting search and recommendation performance in cold-start scenarios. LLM-Rec [44] investigates various prompting strategies to generate augmented input text from GPT-3 (text-davinci-003), which improves the recommendation capabilities. Another work [45] utilizes InstructGPT (175B) [48] for authoring synthetic narrative queries from user-item interactions and train retrieval models for narrativedriven recommendation on synthetic data. TagGPT [33] provides a system-level solution of tag extraction and multi-modal tagging in a zero-shot fashion equipped with GPT-3.5 (gpt-3.5-turbo). Although these methods have made early attempts at utilizing LLMs, they either simply use LLMs as fixed encoders to convert original texts into dense vectors without generating additional textual knowledge, or do not make specific designs to incorporate the generated textual information into traditional recommender systems. This may lead to the instability of RS due to the noise or high dimension of LLM embeddings. In this paper, we propose factorization prompting to break down the complex preference reasoning problem into several key factors to generate open-world knowledge, avoiding the compositional gap. Then, we devise a hybrid-expert adaptor module to reduce dimensionality and ensemble multiple experts for robust knowledge learning, thus increasing the reliability and availability of the generated knowledge. The above two stages can be preprocessed and prestored for fast training and inference, which avoids the large inference latency when using LLMs in RSs. Now, our proposed KAR has been deployed in Huawei news and music recommendation platforms and has gained significant improvements.\n# 3 PRELIMINARIES\nIn this section, we formulate the recommendation task and introduce the notations. The recommendation task is generally formulated as a binary classification problem over multi-field categorical data. The dataset is denoted as D = {(\ud835\udc651,\ud835\udc661), . . . , (\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56), . . . , (\ud835\udc65\ud835\udc5b,\ud835\udc66\ud835\udc5b)} where \ud835\udc65\ud835\udc56represents the categorical features for the \ud835\udc56-th instance and \ud835\udc66\ud835\udc56denotes the corresponding binary label (0 for no-click and 1 for click). Usually, \ud835\udc65\ud835\udc56contains sparse one-hot vectors from multiple fields, such as item ID and genre. We can represent the feature as \ud835\udc65\ud835\udc56= [\ud835\udc65\ud835\udc56,1,\ud835\udc65\ud835\udc56,2, . . . ,\ud835\udc65\ud835\udc56,\ud835\udc39] with \ud835\udc39being the number of field and \ud835\udc65\ud835\udc56,\ud835\udc58, \ud835\udc58= 1, . . . , \ud835\udc39being the feature of the corresponding field. Recommendation models usually aims to learn a function \ud835\udc53(\u00b7) with parameters \ud835\udf03that can accurately predict the click probability \ud835\udc43(\ud835\udc66\ud835\udc56= 1|\ud835\udc65\ud835\udc56) for each sample \ud835\udc65\ud835\udc56, that is \u02c6\ud835\udc66\ud835\udc56= \ud835\udc53(\ud835\udc65\ud835\udc56;\ud835\udf03). In practice, industrial recommender systems are confronted with massive users and items. Thus, their recommendation is usually divided into multiple stages, i.e., candidate generation, ranking, and reranking [43], where different models are used to narrow down the relevant items. However, these classical models are typically trained on a specific recommendation dataset (i.e., a closed system), overlooking the potential benefits of accessing open-world knowledge.\n# 4 METHODOLOGY\nWe first provide an overview of our proposed Open-World Knowledge Augmented Recommendation Framework with Large Language Models, and then elaborate on the details of each component.\n# 4.1 Overview\nTo extract open-world knowledge from LLMs and incorporate it into RSs, we design KAR, as shown in Figure 2. This framework is model-agnostic and consists of the following three stages: Knowledge Reasoning and Generation Stage leverages our designed factorization prompting to extract recommendation-relevant knowledge from LLMs. We first decompose the complex reasoning tasks by identifying major factors that determine user preferences and item characteristics. Then according to each factor, LLMs are required to generate (i) reasoning knowledge on user preferences, and (ii) factual knowledge about items. Thus, we can obtain the openworld knowledge beyond the original recommendation dataset. Knowledge Adaptation Stage converts textual open-world knowledge into compact and relevant representations suitable for recommendation, bridging the gap between LLMs and RSs. First, the reasoning and factual knowledge obtained from LLMs are encoded into dense representations by a knowledge encoder. Next, a hybridexpert adaptor is designed to transform the representations from the semantic space3 to the recommendation space. In this way, we obtain the reasoning augmented vector for user preferences and the fact augmented vector for each candidate item. Knowledge Utilization Stage integrates the reasoning and fact augmented vectors into an existing recommendation model, enabling it to leverage both domain knowledge and open-world knowledge during the recommendation process. The knowledge generation and encoding are conducted through preprocessing. The hybrid-expert adaptor and recommendation model are jointly trained in an end-to-end manner.\n# 4.2 Knowledge Reasoning and Generation\nAs the model size scales up, LLMs can encode a vast array of world knowledge and have shown emergent behaviors such as the reasoning ability [25, 53]. This opens up new possibilities for incorporating reasoning knowledge for user preferences and factual knowledge for candidate items in recommendation systems. However, it is non-trivial to extract the reasoning knowledge and corresponding factual knowledge from LLMs due to the following two challenges. Considering the reasoning knowledge, according to [52], LLMs often suffer from the compositional gap where the model fails at generating the correct answer to the compositional question but can correctly answer all its sub-questions. User\u2019s clicks on items are motivated by multiple key aspects and user\u2019s interests are diverse and multifaceted, which involve multiple reasoning steps. To this end, LLMs may not be able to directly produce accurate reasoning knowledge. Expecting LLMs to provide precise recommendations in one step as in previous work [13, 23, 41] might be overly ambitious. As for the factual knowledge, LLMs contain massive world knowledge, yet not all of it is useful for recommendation. When the request to an LLM is too general, the generated factual knowledge\n3the embedding space from language models\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f18/3f18a628-2384-440c-ad33-b159c9a916d6.png\" style=\"width: 50%;\"></div>\nFigure 2: The overall framework of KAR, consisting of knowledge reasoning and generation stage, knowledge adaptation stage and knowledge utilization stage. Knowledge reasoning and generation stage leverages our designed factorization prompting to extract the reasoning and factual knowledge from LLMs. Knowledge adaptation stage converts textual open-world knowledge into compact and the reasoning and fact augmented representations suitable for recommendation. Knowledge utilization stage integrates the reasoning and fact augmented vectors into an existing recommendation model.\n<div style=\"text-align: center;\">Figure 2: The overall framework of KAR, consisting of knowledge reasoning and generation stage, knowledge adaptation stage and knowledge utilization stage. Knowledge reasoning and generation stage leverages our designed factorization prompting to extract the reasoning and factual knowledge from LLMs. Knowledge adaptation stage converts textual open-world knowledge into compact and the reasoning and fact augmented representations suitable for recommendation. Knowledge utilization stage integrates the reasoning and fact augmented vectors into an existing recommendation model.</div>\nmay be correct but useless, as it may not align with the inferred user preferences. For example, an LLM may infer that a user may prefer highly acclaimed movies that have received multiple awards, while the generated factual knowledge is about the storyline of the target movie. This mismatch between preference reasoning knowledge and item factual knowledge may limit the performance of RSs. Therefore, inspired by the success of Factorization Machines [57] in RSs, we design factorization prompting to explicitly \"factorize\" user preferences into several major factors for effectively extracting the open-world knowledge from LLMs. With the factors incorporated into preference reasoning prompt, the complex preference reasoning problem can be broken down into simpler subproblems for each factor, thus alleviating the compositional gap of LLMs. Besides, we also design item factual prompt which utilizes those factors to extract factual knowledge relevant to user preferences. This ensures the generated reasoning knowledge and factual knowledge are aligned for the effective utilization in RSs.\n4.2.1 Scenario-specific Factors. The factors determining user preferences may vary for different recommendation scenarios. To determine the factors for different scenarios, we rely on a combination of interactive collaboration with LLMs and expert opinions. For example, in movie recommendation, given a prompt \"List the important factors or features that determine whether a user will be interested in a movie\", LLMs can provide some potential factors. Then, we involve human experts to confirm and refine the outputs to acquire the final scenario-specific factors for movie recommendation \u2014 including genre, actors, directors, theme, mood, production quality, and critical acclaim. Similarly, in news recommendation, we may obtain the factors like topic, source, region, style, freshness, clarity, and impact. This collaborative process between LLMs and experts ensures that the chosen factors encompass the critical dimensions of user preference and item characteristics for each scenario. The specification of these factors is required only once for each scenario, showing that the proposed method can be easily generalized to different scenarios with little human intervention.\nKnowledge Utilization\n4.2.2 LLM as Preference Reasoner & Knowledge Provider. After obtaining the scenario-specific factors, we introduce them into our prompt engineering. To extract reasoning and factual knowledge from the open world, we propose to apply the LLM as a preference reasoner to infer user preferences, and a knowledge provider to acquire external factual knowledge for candidate items. Therefore, we design two types of prompts accordingly: preference reasoning prompt and item factual prompt, as illustrated in Figure 3. Preference reasoning prompt is constructed with the user\u2019s profile description, behavior history, and scenario-specific factors. Figure 3 shows an example of the prompt and real response from the LLM, where the user profile description and behavior history provide LLM with the necessary context and user-specific information to understand the user\u2019s preferences. Scenario-specific factors can instruct the LLM to analyze user preference from different facets and allow the LLM to recall the relevant knowledge more effectively and comprehensively. For example, in the factor of genre, the LLM infers user preference for genres such as thriller, comedy, and animation based on user\u2019s positive ratings on thriller movies like What Lies Beneath, Scream, as well as comedy animations like Toy Story and Aladdin. With the designed prompt, LLM can successfully analyze the user\u2019s preferences toward corresponding factors, which is beneficial for recommendations. Item factual prompt is designed to fill the knowledge gap between the candidate items and the generated reasoning knowledge. Since the dataset in RS may lack relevant knowledge about scenario-specific factors from items, we need to extract corresponding knowledge from LLM to align the generated user and item knowledge. As illustrated in Figure 3, an item prompt consists of two parts \u2013 the target item description and the scenario-specific factors. This prompt can guide LLM in compensating for the missing knowledge within the dataset. For instance, in Figure 3, LLM supplements Roman Holiday with \"a light and playful tone\", which is rarely recorded in the datasets. In this way, LLM provides external knowledge that aligns with user preferences, allowing for more accurate and personalized recommendations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f10e/f10e8359-f01c-4506-817b-a37b4a7f9430.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26ca/26ca453f-4e77-4177-860d-278be0da37b2.png\" style=\"width: 50%;\"></div>\n# Figure 3: Example prompts for KAR. The green, purple, and yellow text bubbles represent the promp be filled in the template, and the response generated by LLMs, respectively (some text has been omitte\n<div style=\"text-align: center;\">Figure 3: Example prompts for KAR. The green, purple, and yellow text bubbles represent the prompt template, the content t be filled in the template, and the response generated by LLMs, respectively (some text has been omitted due to the page limits</div>\n# mpts for KAR. The green, purple, and yellow text bubbles represent the prompt template, the content to , and the response generated by LLMs, respectively (some text has been omitted due to the page limits).\nBy combining the two kinds of prompts, we enable the LLM to act as both a preference reasoner and a knowledge provider, thereby extracting the open-world knowledge from LLMs.\n# 4.3 Knowledge Adaptation\nThe knowledge generated by LLMs presents new challenges in harnessing its potential to assist recommendation models: 1) The knowledge generated by LLMs is usually in the form of text, which cannot be directly leveraged by traditional RSs that typically process categorical features. 2) Even if some LLMs are open-sourced, the decoded outputs are usually large dense vectors (e.g., 4096 for each token) and lie in a semantic space that differs significantly from the recommendation space. 3) The generated knowledge may contain noise or unreliable information [29]. To address these challenges, we have devised two modules: a knowledge encoder and a hybrid-expert adaptor. The knowledge encoder module encodes the generated textual knowledge into dense vectors and aggregates them effectively. The hybrid-expert adaptor converts dense vectors from the semantic space to the recommendation space. It tackles dimensionality mismatching and allows for noise mitigation. Thus, the knowledge adaptation stage increases the reliability and availability of the generated knowledge and bridges the gap between LLMs and RSs.\n4.3.1 Knowledge Encoder. To harness the potential of textual knowledge generated by LLMs, we employ a knowledge encoder, e.g., BERT [31], to obtain the encodings for each token within the text. Then, we require an aggregation process that combines each token to generate the preference reasoning representation \ud835\udc5f\ud835\udc5d \ud835\udc56\u2208R\ud835\udc5aand the item factual representation \ud835\udc5f\ud835\udf04 \ud835\udc56\u2208R\ud835\udc5aof size \ud835\udc5aas follows\n(1)\nwhere \ud835\udc58\ud835\udc59\ud835\udc54\ud835\udc5d \ud835\udc56and \ud835\udc58\ud835\udc59\ud835\udc54\ud835\udf04 \ud835\udc56denote the textual reasoning knowledge and factual knowledge generated by LLMs of the \ud835\udc56-th instance in the dataset. Here, various aggregation functions can be employed, such as the representation of the [CLS] token and average pooling. In practice, we primarily adopt average pooling. Note that the knowledge encoder is devised for situations where we only have access to\nthe textual outputs of LLMs. If the dense vector outputs from LLM are available, a separate knowledge encoder can be eliminated.\n4.3.2 Hybrid-expert Adaptor. To effectively transform and compact the attained aggregated representations from the semantic space to the recommendation space, we propose a hybrid-expert adaptor module. The aggregated representations capture diverse knowledge from multiple aspects, so we employ a structure that mixes shared and dedicated experts, inspired by the Mixture of Experts (MoE) [27] approach. This allows us to fuse knowledge from different facets and benefits from the inherent robustness offered by multiple experts. In particular, to fully exploit the shared information of the preference reasoning representation and the item factual representation, we have designed both shared experts and dedicated experts for each kind of representation. The shared experts capture the common aspects, such as shared features, patterns, or concepts, that are relevant to both preference reasoning and item factual knowledge. Reasoning and factual representations also have their dedicated sets of experts to capture the unique characteristics specific to the reasoning or factual knowledge. Mathematically, denote S\ud835\udc60, S\ud835\udc5d, and S\ud835\udf04as the sets of shared experts and dedicated experts for preference reasoning and item factual knowledge with the expert number of \ud835\udc5b\ud835\udc60, \ud835\udc5b\ud835\udc5dand \ud835\udc5b\ud835\udf04. The output is the reasoning augmented vector \u02c6\ud835\udc5f\ud835\udc5d \ud835\udc56\u2208R\ud835\udc5e and the fact augmented vector \u02c6\ud835\udc5f\ud835\udf04 \ud835\udc56\u2208R\ud835\udc5eof size \ud835\udc5e(\ud835\udc5eis much less than the original dimension \ud835\udc5a), which are calculated as follows\n(2)\nwhere\ud835\udc54\ud835\udc5d(\u00b7) and\ud835\udc54\ud835\udf04(\u00b7) are the gating networks for preference reasoning and item factual representations, and their outputs \ud835\udefc\ud835\udc5d \ud835\udc56and \ud835\udefc\ud835\udf04 \ud835\udc56 are of size \ud835\udc5b\ud835\udc60+\ud835\udc5b\ud835\udc5dand \ud835\udc5b\ud835\udc60+\ud835\udc5b\ud835\udf04. Here \ud835\udc52(\u00b7) denotes the expert network, and \ud835\udefc\ud835\udc5d \ud835\udc56,\ud835\udc52and \ud835\udefc\ud835\udf04 \ud835\udc56,\ud835\udc52are the weights of expert \ud835\udc52(\u00b7) generated by the gating network for preference and item, respectively. Here, each expert network \ud835\udc52(\u00b7) is designed as Multi-Layer Perceptron (MLP), facilitating dimensionality reduction and space transformation.\nTowards Open-World Recommendation with Knowledge Augmentation from Large Language Mo\n# 4.4 Knowledge Utilization\nOnce we have obtained the reasoning augmented vector and the fact augmented vector, we can then incorporate them into backbone recommendation models. In this section, we explore a straightforward approach where these augmented vectors are directly treated as additional input features. Specifically, we use them as additional feature fields in recommendation models, allowing them to explicitly interact with other features. During training, the hybrid-expert adaptor module is jointly optimized with the backbone model to ensure that the transformation process adapts to the current data distribution. Generally, KAR can be formulated as\n(3)\nwhich is enhanced by the the reasoning augmented vector \u02c6\ud835\udc5f\ud835\udc5d \ud835\udc56and the fact augmented vector \u02c6\ud835\udc5f\ud835\udf04 \ud835\udc56. Importantly, KAR only modifies the input of the backbone model and is independent of the design and loss function of backbone model, so it is flexible and compatible with various backbone model designs. Furthermore, it can be extended to various recommendation tasks, such as sequential recommendation and direct recommendation, by simply adding two augmented vectors in the input. By incorporating the knowledge augmented vectors, KAR combines both the open-world knowledge and the recommendation domain knowledge in a unified manner to provide more informed and personalized recommendations.\n# 4.5 Speed-up Approach\nOur proposed KAR framework adopts LLMs to generate reasoning knowledge for user preferences and factual knowledge for candidate items. Due to the immense scale of the model parameters, the inference of LLMs takes extensive computation time and resources, and the inference time may not meet the latency requirement in real-world recommender systems with large user and item sets. To address this, we employ an acceleration strategy to prestore knowledge representations \ud835\udc5f\ud835\udc5d \ud835\udc56and \ud835\udc5f\ud835\udf04 \ud835\udc56generated by the knowledge encoder or the LLM into a database. As such, we only use the LLM and knowledge encoder once before the training of backbone models. During the training and inference of the backbone model, relevant representations are retrieved from the database. Besides, the efficiency of offline knowledge generation with LLM can further be enhanced via quantization or hardware acceleration techniques. If we have stricter requirements for inference time or storage efficiency, we can detach the adaptor from the model after training and further prestore augmented vectors i.e., \u02c6\ud835\udc5f\ud835\udc5d \ud835\udc56and \u02c6\ud835\udc5f\ud835\udf04 \ud835\udc56, for inference. The dimension of the augmented vectors (e.g., 32) is usually much smaller than that of the knowledge representations (e.g., 4096), which improves the storage efficiency. Additionally, prestoring the augmented vectors reduces the inference time to nearly the same as the original backbone model, and we have provided experimental verification in Section 5.5. In particular, assume the inference time complexity of the backbone model is \ud835\udc42(\ud835\udc53(\ud835\udc5b,\ud835\udc5a)), where \ud835\udc5bis the number of fields and \ud835\udc5ais the embedding size. The polynomial function \ud835\udc53(\ud835\udc5b,\ud835\udc5a) varies depending on different backbone models. With KAR, the inference time complexity is \ud835\udc42(\ud835\udc53(\ud835\udc5b+2,\ud835\udc5a)) = \ud835\udc42(\ud835\udc53(\ud835\udc5b,\ud835\udc5a)), which is equivalent to the complexity of the original model. Since item features are relatively fixed and do not change frequently, it is natural and feasible to prestore the item factual knowledge for further use. Moreover, user behaviors evolve over time,\nmaking it challenging for LLMs to provide real-time reasoning knowledge about behaviors. However, considering that long-term user preferences are relatively stable, and the backbone model already emphasizes modeling recent user behaviors, it is unnecessary to require LLMs to have access to real-time behaviors. Therefore, LLM can infer long-term preferences based on users\u2019 long-term behaviors, allowing for conveniently prestoring the generated knowledge without frequent updates. The inference overhead of LLMs can also be significantly reduced. The backbone models can capture ever-changing short-term preferences with timely model updates. This can take better advantage of both LLMs and recommendation models. Similar to common practice for cold start users or items [74], we use default vectors when encountering new users or items at inference time. Subsequently, we will generate the knowledge for those new users and items offline and add them to the database.\n# 5 EXPERIMENT\nTo gain more insights into KAR, we tend to address the following research questions (RQs) in this section. \u2022 RQ1: What improvements can KAR bring to backbone models on different tasks, such as CTR prediction and reranking? \u2022 RQ2: How does KAR perform compared with other PLM-based baseline methods? \u2022 RQ3: Does the knowledge from LLM outperform other methods of knowledge, such as knowledge graph? \u2022 RQ4:Does KAR gain performance improvement when deployed online? \u2022 RQ5: How do the reasoning knowledge and factual knowledge generated by the LLM contribute to performance improvement? \u2022 RQ6: How do different knowledge adaptation approaches impact the performance of KAR? \u2022 RQ7: Does the acceleration strategy, preprocessing and prestorage, enhance the inference speed? By answering these questions, we aim to comprehensively evaluate the performance and versatility of our proposed framework.\n# 5.1 Setup\n5.1.1 Dataset. Our experiments are conducted on public datasets, MovieLens-1M4 and Amazon5. MovieLens-1M contains 1 million ratings provided by 6000 users for 4000 movies. Following the data processing similar to DIN [79], we convert the ratings into binary labels by labeling ratings of 4 and 5 as positive and the rest as negative. The data is split into training and testing sets based on user IDs, with 90% assigned to the training set and 10% to the testing set. The dataset contains user features like age, gender, occupation, and item features like item ID and category. The input to the models are user features, user behavior history (the sequence of viewed movies with their ID, category, and corresponding ratings), and target item features. Amazon-Book [46] is the \u201cBooks\u201d category of the Amazon Review Dataset. After filtering out the less-interacted users and items, we remain 11, 906 users and 17, 332 items with 1, 406, 582 interactions. The preprocessing is similar to MovieLens-1M, with the difference being the absence of user features. Additionally, ratings of 5 are regarded as positive and the rest as negative.\n4https://grouplens.org/datasets/movielens/1m/ 5https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/\nwork, various tasks and models in traditional ID-based recommendation can serve as the backbone model, taking as input the knowledge-augmented vectors generated by KAR. Here, we select two crucial recommendation tasks: CTR prediction and reranking, to validate the effectiveness of KAR across various tasks. CTR prediction aims to anticipate how likely a user is to click on an item, usually used in the ranking stage of recommendation. Reranking is to reorder the items from the previous ranking stage and derive a list that yields more utility and user satisfaction [43]. We choose 9 representative CTR models as our backbone models, which can be categorized into user behavior models and feature interaction models. User Behavior Models emphasize modeling sequential dependencies of user behaviors. DIN [79] utilizes attention to model user interests dynamically with respect to a certain item. DIEN [78] extends DIN by introducing an interest evolving mechanism to capture the dynamic evolution of user interests over time. Feature Interaction Models focus on modeling feature interactions between different feature fields. DeepFM [16] is a classic CTR model that combines factorization machine (FM) and neural network to capture low-order and high-order feature interactions. xDeepFM [37] leverages the power of both deep network and Compressed Interaction Network to generate feature interactions at the vector-wise level. DCN [65] incorporates cross-network architecture and the DNN model to learn the bounded-degree feature interactions. DCNv2 [66] is an improved framework of DCN which is more practical in large-scale industrial settings. FiBiNet [26] can dynamically learn the feature importance by Squeeze-Excitation network and fine-grained feature interactions by bilinear function. FiGNN [36] converts feature interactions into modeling node interactions on the graph for modeling feature interactions in an explicit way. AutoInt [59] adopts a self-attentive neural network with residual connections to model the feature interactions explicitly. As fo reranking task, we implement the state-of-the-art models, e.g., DLCM [1], PRM [51], SetRank [50], and MIR [70], as backbone models. DLCM [1] first applies GRU to encode and rerank the top results. PRM [51] employs self-attention to model the mutual influence between any pair of items and users\u2019 preferences. SetRank [50] learns permutation-equivariant representations for the inputted items via self-attention. MIR [70] models the set-to-list interactions between candidate set and history list with personalized long-short term interests.\n5.1.3 PLM-based Baselines. As for baselines, we compare KAR with methods that leverage pretrained language model to enhance recommendation, such as P5 [14], UniSRec [22], VQRec [21], TALLRec [2], and LLM2DIN [19]. P5 [14] is a text-to-text paradigm that unifies recommendation tasks and learns different tasks with the same language modeling objective during pretraining. UniSRec [22] designs a universal sequence representation learning approach for sequential recommenders, which introduces contrastive pretraining tasks to effective transfer across scenarios. VQ-Rec [21] uses Vector-Quantized item representations and a text-to-codeto-representation scheme, achieving effective cross-domain and cross-platform sequential recommendation. TALLRec [2] finetunes LLaMa-7B [60] with a LoRA architecture on recommendation tasks\nand enhances the recommendation capabilities of LLMs in few-shot scenarios. In our experiment, we implement TALLRec with LLaMa2-7B-chat6, since it has better performance and ability of instruction following. According to [19], we design LLM2DIN which initializes DIN with item embeddings obtained from chatGLM[10]. We utilize the publicly available code of these three models and adapt the model to the CTR task with necessary minor modifications. We also align the data and features for all the methods to ensure fair comparisons.\n5.1.4 Evaluation Metrics. On CTR prediction task, we employ widely used AUC (Area under the ROC curve) and LogLoss (binary crossentropy loss) as evaluation metrics following [16, 59, 66, 79]. A higher AUC value or a lower Logloss value, even by a small margin (e.g., 0.001), can be viewed as a significant improvement in CTR prediction performance, as indicated by previous studies [37, 66]. As for reranking task, several widely used metrics, NDCG@K [28] and MAP@K [72], are adopted, following previous work [1, 51, 70]. 5.1.5 Implementation Details. We utilize API of a widely-used LLM for generating reasoning and factual knowledge. Then, ChatGLM [10] is employed to encode the knowledge, followed by average pooling as the aggregation function in Eq. (1). Each expert in the hybrid-expert adaptor is implemented as an MLP with a hidden layer size of [128, 32]. The number of experts varies slightly across different backbone models, typically with 2-5 shared experts and 2-6 dedicated experts. We keep the embedding size of the backbone model as 32, and the output layer MLP size as [200, 80]. Other parameters, such as batch size and learning rate, are determined through grid search to achieve the best results. For fair comparisons, the parameters of the backbone model and the baselines are also tuned to achieve their optimal performance.\n5.1.5\n# 5.2 Effectiveness Comparison\n5.2.1 Improvement over Backbone Models (RQ1). On CTR prediction task, we implement our proposed KAR upon 9 representative CTR models, and the results are shown in Table 1. From the table, we can have the following observations: (i) Applying KAR significantly improves the performance of backbone CTR models. For example, when using FiBiNet as the backbone model on MovieLens1M, KAR achieves a 1.49% increase in AUC and a 2.27% decrease in LogLoss, demonstrating the effectiveness of incorporating openworld knowledge from LLMs into RSs. (ii) As a model-agnostic framework, KAR can be applied to various types of baseline models, whether focusing on feature interaction or behavior modeling. With the equipment of KAR, the selected 9 representative CTR models on two datasets all achieve an AUC improvement of about 1-1.5%, indicating the universality of the KAR. (iii) KAR shows more remarkable improvement in feature interaction models compared to user behavior models. This may be because the knowledge augmented vectors generated by KAR are utilized more effectively by the feature interaction layer than the user behavior modeling layer. The dedicated feature interaction design may better exploit the information contained in the knowledge vectors. To investigate KAR\u2019s compatibility on other tasks, e.g., reranking, we incorporate KAR into the state-of-the-art reranking models.\n6https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n<div style=\"text-align: center;\">Table 1: Comparison between KAR and backbone CTR prediction models.</div>\nBackbone\nmodel\nMovieLens-1M\nAmazon-Books\nAUC\nLogLoss\nAUC\nLogLoss\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nDCNv2\n0.7830\n0.7935*\n1.34%\n0.5516\n0.5410*\n1.92%\n0.8269\n0.8350*\n0.98%\n0.4973\n0.4865*\n2.17%\nDCNv1\n0.7828\n0.7927*\n1.28%\n0.5528\n0.5411*\n2.12%\n0.8268\n0.8348*\n0.97%\n0.4973\n0.4869*\n2.11%\nDeepFM\n0.7824\n0.7919*\n1.22%\n0.5518\n0.5432*\n1.56%\n0.8269\n0.8347*\n0.94%\n0.4969\n0.4873*\n1.93%\nFiBiNet\n0.7820\n0.7936*\n1.49%\n0.5531\n0.5405*\n2.27%\n0.8269\n0.8351*\n0.99%\n0.4973\n0.4870*\n2.07%\nAutoInt\n0.7821\n0.7931*\n1.40%\n0.5520\n0.5430*\n1.62%\n0.8262\n0.8357*\n1.16%\n0.4981\n0.4863*\n2.37%\nFiGNN\n0.7832\n0.7935*\n1.32%\n0.5510\n0.5437*\n1.33%\n0.8270\n0.8352*\n0.99%\n0.4977\n0.4870*\n2.14%\nxDeepFM\n0.7823\n0.7926*\n1.32%\n0.5520\n0.5420*\n1.81%\n0.8271\n0.8351*\n0.97%\n0.4971\n0.4866*\n2.10%\nDIEN\n0.7853\n0.7953*\n1.27%\n0.5494\n0.5394*\n1.83%\n0.8307\n0.8391*\n1.01%\n0.4926\n0.4812*\n2.32%\nDIN\n0.7863\n0.7961*\n1.24%\n0.5486\n0.5370*\n2.10%\n0.8304\n0.8418*\n1.38%\n0.4937\n0.4801*\n2.77%\ndenotes statistically significant improvement over backbone CTR prediction models (t-test with-value 0.05).\n<div style=\"text-align: center;\">arison of KAR and backbone reranking models on Amazon-Books datas</div>\nBackbone\nModel\nMAP@3\nMAP@7\nNDCG@3\nNDCG@7\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nbase\nKAR\nImprov.\nDLCM\n0.6365\n0.6654*\n4.54%\n0.6247\n0.6512*\n4.24%\n0.5755\n0.6109*\n6.15%\n0.6891\n0.7142*\n3.64%\nPRM\n0.6488\n0.6877*\n6.00%\n0.6359\n0.6722*\n5.71%\n0.5909\n0.6379*\n7.95%\n0.6983\n0.7312*\n4.71%\nSetRank\n0.6509\n0.6711*\n3.10%\n0.6384\n0.6538*\n2.41%\n0.5947\n0.6137*\n3.19%\n0.7006\n0.7164*\n2.26%\nMIR\n0.7178\n0.7241*\n0.88%\n0.7011\n0.7078*\n0.96%\n0.6747\n0.6837*\n1.33%\n0.7549\n0.7597*\n0.64%\ndenotes statistically significant improvement over backbone reranking models (t-test with-value 0.05).\nThe results on the Amazon-Books dataset are presented in Table 2, from which the following observations can be made: (i) KAR significantly enhances the performance of backbone reranking models. For example, when PRM is employed as the backbone, KAR achieves a remarkable increase of 5.71% and 4.71% in MAP@7 and NDCG@7. (ii) KAR demonstrates more pronounced improvements in methods that do not involve history modeling, such as DLCM, PRM, and SetRank. The user preference knowledge provided by KAR is particularly advantageous for these methods. However, in MIR, which adequately explores the relationship between history and candidates, the enhancement is slightly smaller.\n5.2.2 Improvement over Baselines (RQ2). Next, we compare KAR with recent baselines using language models or sequence representation pretraining on CTR task. The results are presented in Table 3, from which we make the following observations: (i) KAR significantly outperforms models based on pretrained language models. For instance, with DIN as the backbone on Amazon-Books, KAR achieves a 0.91% improvement in AUC and a 1.27% improvement in LogLoss over the strongest baseline TALLRec. (ii) When integrating PLMs into RSs, the improvements brought by smaller PLMs are relatively modest. For instance, UniSRec, VQ-Rec, and P5, based on PLMs with parameters less than one billion, tend to exhibit poorer performance, even worse than the baseline DIN. (iii) Conversely, leveraging LLMs can lead to more substantial improvements, as seen in TALLRec and KAR. However, achieving effective enhancements with LLM embedding as initialization proves challenging,\ne.g., there is little difference between the results of LLM2DIN and DIN.\n<div style=\"text-align: center;\">Table 3: Comparison between KAR and baselines.</div>\nModel\nBackbone\nPLM\nMovieLens-1M\nAmazon-Books\nAUC\nLogLoss\nAUC\nLogLoss\nUnisRec\nBERT-110M\n0.7702\n0.5641\n0.8196\n0.5063\nVQ-Rec\nBERT-110M\n0.7707\n0.5641\n0.8226\n0.5025\nP5\nT5-223M\n0.7790\n0.5543\n0.8333\n0.4908\nLLM2DIN\nChatGLM-6B\n0.7874\n0.5473\n0.8307\n0.4930\nTALLRec\nLLaMa2-7B\n0.7892\n0.5451\n0.8342\n0.4862\nbase(DIN)\nN/A\n0.7863\n0.5486\n0.8304\n0.4937\nKAR(DIN) ChatGLM-6B 0.7961*\n0.5370*\n0.8418*\n0.4801*\n\u2217denotes statistically significant improvement over the second best baselines which is underlined (t-test with \ud835\udc5d-value < 0.05).\n5.2.3 Improvement over Other Knowledge (RQ3). Finally, in Figure 4, we compare knowledge from LLMs and other sources, such as knowledge graph (KG), with DCNv1, DeepFM, and DIN as the backbone on MovieLens-1M dataset. We utilize a knowledge graph from LODrecsys [8], which maps items of MovieLens-1M to DBPedia entities. Then, the entity embedding of each item is extracted following KTUP [5] and used as an additional feature for the backbone model. The legend \"None\" denotes the backbone model without knowledge enhancement, while \"KG\", \"LLM\", and \"Both\" represent\nthe backbone model enhanced by knowledge from KG, LLM, and both sources, respectively.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d360/d360792e-ad45-4dfe-acac-a9a59d3a07e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Comparison between knowledge from knowledge graph and LLM on MovieLens-1M dataset.</div>\nFrom Figure 4, we notice that knowledge from both KG and LLM can bring performance improvements; however, the enhancement of KG is much smaller compared to that of LLM. This might be attributed to the fact that KG only possesses manually annotated item-side knowledge while it lacks reasoning knowledge for user preferences. According to our analysis in the next subsection 5.4.1, reasoning knowledge for user preference usually contributes more gains compared to item factual knowledge. Furthermore, the simultaneous utilization of the two kinds of knowledge does not exhibit significant improvement over using LLM alone. This suggests that LLM may already encompass the typical knowledge within KG, making knowledge derived solely from LLM sufficient.\n# 5.3 Deployment & Online A/B Test (RQ4)\nTo validate the effectiveness of KAR, we conducted two online experiments on Huawei\u2019s news and music platforms, respectively. On the news platform, the experimental group, where we deployed KAR, utilized Huawei\u2019s large language model PanGu[73] to generate user preference, followed by retrieving relevant news through vector similarity. The control group utilized the original recall model as our baseline. During the online A/B test, KAR exhibited a 7% improvement on Recall metric compared with the baseline, resulting in significant business benefits. In the music scenario, 10% of users were randomly selected into the experimental group and another 10% were in the control group. Both groups used the same base model for generating recommendations. The difference lies in their inputs, where the input of the experimental group included the knowledge representation augmented by KAR. To enhance the storage efficiency, we also employed PCA to reduce the dimension of representation to 64. In a 7-day online A/B test, KAR demonstrated a 1.7% increase in song play count, a 1.64% increase in the number of devices for song playback, and a 1.57% increase in total duration. This indicates that KAR can be successfully implemented in industrial settings and improve recommendation experience for real-world users.\n# 5.4 Ablation Study\n5.4.1 Reasoning and Factual Knowledge (RQ5). To study the impact of knowledge generated by LLMs, we conduct an ablation study on reasoning and factual knowledge on the Amazon-Books dataset. We select DCNv2, AutoInt, and DIN as backbone models and compare their performance with different knowledge enhancements,\nas shown in Figure 5. The legend \"None\" represents the backbone model without any knowledge enhancement. \"Fact\" and \"Reas\" indicate the backbone models enhanced with factual knowledge on item and reasoning knowledge about user preference, respectively, while \"Both\" represents the joint use of both knowledge types.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c191/c191f4f3-199c-4553-97b3-63c0c9b9c1f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Ablation study about reasoning and factual knowledge on Amazon-Books dataset.</div>\n# Figure 5: Ablation study about reasoning and factual knowledge on Amazon-Books dataset.\nFrom Figure 5, we observe that both reasoning knowledge and factual knowledge can improve the performance of backbone models, with reasoning knowledge exhibiting a larger improvement. This could be attributed to the fact that reasoning knowledge inferred by the LLMs captures in-depth user preferences, thus compensating for the backbone model\u2019s limitations in reasoning underlying motives and intentions. Additionally, the joint use of both reasoning and factual enhancements outperforms using either one alone, even achieving a synergistic effect where 1 + 1 > 2. One possible explanation is that reasoning knowledge contains external information that is not explicitly present in the raw data. When used independently, this external knowledge could not be matched with candidate items. However, combining the externally generated factual knowledge on items from the LLMs aligned with reasoning knowledge allows RSs to gain a better understanding of items according to user preferences.\n# Table 4: Impact of different knowledge encoders on MoiveLens-1M dataset.\n<div style=\"text-align: center;\">Table 4: Impact of different knowledge encoders on MoiveLens-1M dataset.</div>\nVariants\nBERT\nChatGLM\nAUC\nLogLoss\nAUC\nLogLoss\nbase(DIN)\n0.7863\n0.5486\n0.7863\n0.5486\nKAR(LR)\n0.7589\n0.5811\n0.7720\n0.5674\nKAR(MLP)\n0.7754\n0.5588\n0.7816\n0.5559\nKAR-MLP\n0.7934\n0.5411\n0.7939\n0.5401\nKAR-MoE\n0.7946\n0.5391\n0.7950\n0.5394\nKAR\n0.7947\n0.5402\n0.7961\n0.5370\n5.4.2 Knowledge Encoders and Semantic Transformation (RQ6). We employ two different language models, BERT [31] and ChatGLM [10], to investigate the impact of different knowledge encoders on model performance. Additionally, we design several variants to demonstrate how the representations generated by knowledge encoders are utilized. KAR(LR) applies average pooling to token representations from the knowledge encoder and directly feeds the result into a linear layer to obtain prediction scores, without\nTable 5: Impact of different knowledge encoders on AmazonBooks dataset.\nTable 5: Impact of different knowledge encoders on Amazon-\nVariants\nBERT\nChatGLM\nAUC\nLogLoss\nAUC\nLogLoss\nbase(DIN)\n0.8304\n0.4937\n0.8304\n0.4937\nKAR(LR)\n0.7375\n0.5861\n0.7560\n0.5718\nKAR(MLP)\n0.7424\n0.5834\n0.7571\n0.5763\nKAR-MLP\n0.8357\n0.4880\n0.8370\n0.4843\nKAR-MoE\n0.8371\n0.4841\n0.8388\n0.4823\nKAR\n0.8374\n0.4843\n0.8418\n0.4801\nutilizing a backbone CTR model. KAR(MLP) replaces the linear layer of KAR(LR) with an MLP. KAR-MLP and KAR-MoE replace the hybrid-expert adaptor with an MLP and a Mixture-of-Experts (MoE), respectively. The original KAR and the two variants, KARMLP and KAR-MoE, all adopt DIN as the backbone model. Table 4 and 5 presents their performance, from which we draw the following conclusions. Firstly, we can observe that, overall, variants with ChatGLM as the knowledge encoder outperform those with BERT. The performance of KAR(LR) and KAR(MLP) can be considered as a measure of the quality of the encoded representations, since they directly adopt the representations for prediction. Considering KAR(LR) and KAR(MLP), the superior performance of ChatGLM over BERT indicates that ChatGLM performs better in preserving the information within knowledge from LLMs, which may be attributed to the larger size and better text comprehension of ChatGLM (6 billion) compared to BERT (110 million). With ChatGLM on MovieLens-1M, KAR(MLP) is even close to some base CTR model like DeepFM in Table 1, validating the effectiveness of our generated open-world knowledge. Secondly, the performance of KAR benefits from complex semantic transformation structures, but the knowledge encoder also limits it. The results on ChatGLM show that a simple MLP is less effective than MoE and our designed hybrid-expert adaptor outperforms the MoE, indicating that the transformation from semantic space to recommendation space entails a complex network structure. However, with BERT as knowledge encoder, KAR-MoE and KAR-MLP exhibit similar performance, suggesting that information from BERT is limited and using an MoE is sufficient in this case.\n# 5.5 Efficiency Study (RQ7)\nTo quantify the actual time complexity of KAR, we compare the inference time of KAR based on DIN with a widely-used LLM API, strongest baseline TALLRec, and base DIN model in Table 6. For LLM API, we follow the zero-shot user rating prediction in [30] that provides the user viewing history and ratings as prompt and invokes the API to predict user ratings on candidate items. Since this approach does not allow setting a batch size, the table presents the average response time per sample. For KAR, we evaluate the two acceleration strategies as introduced in Section 4.5: KAR\ud835\udc64/\ud835\udc4e\ud835\udc5d\ud835\udc61, where the adaptor participates in the inference stage, and KAR\ud835\udc64/\ud835\udc5c\ud835\udc4e\ud835\udc5d\ud835\udc61, where the adaptor is detached from inference. The experiments of KAR and base model are all conducted on a Tesla V100 with 32G\nmemory, with a batch size of 256. With the same Tesla V100, we also test TALLRec and only showcase the average inference time per sample, since 32G memory cannot handle LLM with batch size of 256. Table 6 presents the average inference time, from which we draw following conclusions. Firstly, adopting LLM for direct inference is not feasible for RSs due to its large computational latency. The response latency of LLM API is 4-6 seconds, which does not meet the real-time requirement of RSs typically demanding a response latency of within 100ms. Even if we finetune a relatively small LLM, such as TALLRec based on LLaMa2-7B, its inference latency is still close to 1s, which is unbearable for industrial scenarios. Secondly, both acceleration methods of KAR achieve an inference time within 100ms, satisfying the low latency requirement. Importantly, if we employ the approach of prestoring reasoning and factual augmented vectors, i.e., KAR\ud835\udc64/\ud835\udc5c\ud835\udc4e\ud835\udc5d\ud835\udc61, the actual inference time is nearly the same as that of the backbone model. This demonstrates the effectiveness of our proposed KAR and acceleration strategies.\n<div style=\"text-align: center;\">Table 6: The comparison of inference time (s).</div>\nModel\nMovieLens-1M Amazon-Books\nLLM API\n5.54\n4.11\nTALLRec\n7.63 \u00d7 10\u22121\n7.97 \u00d7 10\u22121\nKAR\ud835\udc64/ \ud835\udc4e\ud835\udc5d\ud835\udc61\n8.08 \u00d7 10\u22122\n9.39 \u00d7 10\u22122\nKAR\ud835\udc64/\ud835\udc5c\ud835\udc4e\ud835\udc5d\ud835\udc61\n6.64 \u00d7 10\u22123\n1.11 \u00d7 10\u22122\nbase DIN\n6.42 \u00d7 10\u22123\n1.09 \u00d7 10\u22122\n# 6 BROADER IMPACT\nWhen incorporating LLMs into RSs, it is imperative to give serious consideration to privacy and security issues. While LLMs offer remarkable capabilities, they also come with potential risks of compromising user privacy and generating harmful content [3, 49, 68]. We also consider these two issues while designing our proposed framework, KAR. Regarding privacy, KAR leverages LLMs\u2019 reasoning ability and factual knowledge without finetuning LLMs, ensuring that LLMs do not retain or remember user-specific data. Besides, while experiments may involve the use of external APIs on public datasets, real-world implementations usually rely on in-house models, e.g., we utilize Huawei\u2019s own LLM PanGu[73] for online A/B test as described in section 5.3, preventing the leakage of user privacy information through external APIs. Furthermore, compared to methods that directly display LLMgenerated content to users [7, 13, 41], KAR takes a more proactive stance to mitigate concerns about harmful content and hallucination knowledge generated by LLMs. KAR first converts the textual content from LLMs to robust representations and then incorporates those representations into traditional RSs. This integration avoids displaying harmful or misleading content generated by LLMs while enabling us to utilize filtering mechanisms commonly used in traditional RSs to screen out potential harmful item recommendations. Through those measures, KAR strives to deliver robust recommendation performance while safeguarding user data privacy and the security of recommendation content.\n# 7 CONCLUSION\nOur work presents KAR, a framework for effectively incorporating the open-world knowledge into recommender systems by exploiting large language models. KAR identifies two types of critical knowledge from LLMs, the reasoning knowledge on user preferences and the factual knowledge on items, which can be proactively acquired by our designed factorization prompting. A hybrid-expert adaptor is devised to transform the obtained knowledge for compatibility with recommendation tasks. The obtained augmented vectors can then be used to enhance the performance of any recommendation model. Additionally, efficient inference is achieved through preprocessing and prestoring the LLM knowledge. KAR shows superior performance compared to the state-of-the-art methods and is compatible with various recommendation algorithms.\n# REFERENCES\n[1] Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce Croft. 2018. Learning a deep listwise context model for ranking refinement. In The 41st international ACM SIGIR conference on research & development in information retrieval. 135\u2013144. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447 (2023). [3] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tram\u00e8r. 2022. What Does It Mean for a Language Model to Preserve Privacy?. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT \u201922). 2280\u20132292. [4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023). [5] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019. Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences. In The world wide web conference. 151\u2013161. [6] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv preprint arXiv:2205.08084 (2022). [7] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182 (2023). [8] Tommaso Di Noia, Vito Claudio Ostuni, Paolo Tomeo, and Eugenio Di Sciascio. 2016. SPRank: Semantic Path-based Ranking for Top-N Recommendations using Linked Open Data. ACM Transactions on Intelligent Systems and Technology (TIST) (2016). [9] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. 2021. Zeroshot recommender systems. arXiv preprint arXiv:2105.08318 (2021). [10] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320\u2013335. [11] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023). [12] Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al. 2023. Leveraging Large Language Models in Conversational Recommender Systems. arXiv preprint arXiv:2305.07961 (2023). [13] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. arXiv preprint arXiv:2303.14524 (2023). [14] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt and Predict Paradigm (P5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299\u2013315. [15] Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM \u201923). 4595\u20134601. [16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia). 1725\u20131731.\n[17] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549\u20133568. [18] Xiaobo Guo, Shaoshuai Li, Naicheng Guo, Jiangxia Cao, Xiaolei Liu, Qiongxu Ma, Runsheng Gan, and Yunan Zhao. 2023. Disentangled Representations Learning for Multi-target Cross-domain Recommendation. ACM Transactions on Information Systems 41, 4 (2023), 1\u201327. [19] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging Large Language Models for Sequential Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 1096\u20131102. [20] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web. 507\u2013517. [21] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders. In Proceedings of the ACM Web Conference 2023. 1162\u20131171. [22] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 585\u2013593. [23] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large Language Models are Zero-Shot Rankers for Recommender Systems. arXiv preprint arXiv:2305.08845 (2023). [24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. [25] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large Language Models: A Survey. arXiv:2212.10403 [cs.CL] [26] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-through Rate Prediction. In Proceedings of the 13th ACM Conference on Recommender Systems. 169\u2013177. [27] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. Neural Computation 3, 1 (03 1991), 79\u201387. [28] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM Trans. Inf. Syst. (2002), 422\u2013446. [29] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1\u201338. [30] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474 (2023). [31] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 4171\u20134186. [32] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30\u201337. [33] Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023. TagGPT: Large Language Models are Zero-shot Multimodal Taggers. arXiv preprint arXiv:2304.03022 (2023). [34] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text Is All You Need: Learning Language Representations for Sequential Recommendation. arXiv preprint arXiv:2305.13731 (2023). [35] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large Language Models for Generative Recommendation: A Survey and Visionary Discussions. arXiv preprint arXiv:2309.01157 (2023). [36] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 539\u2013548. [37] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1754\u20131763. [38] Guo Lin and Yongfeng Zhang. 2023. Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT. arXiv preprint arXiv:2305.04518 (2023). [39] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023). [40] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. 2021. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823 (2021). [41] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv preprint arXiv:2304.10149\n(2023). [42] Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems. arXiv preprint arXiv:2302.03735 (2023). [43] Weiwen Liu, Yunjia Xi, Jiarui Qin, Fei Sun, Bo Chen, Weinan Zhang, Rui Zhang, and Ruiming Tang. 2022. Neural Re-ranking in Multi-stage Recommender Systems: A Review. arXiv preprint arXiv:2202.06602 (2022). [44] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llmrec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780 (2023). [45] Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large Language Model Augmented Narrative Driven Recommendations. arXiv preprint arXiv:2306.02250 (2023). [46] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188\u2013197. [47] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). https: //doi.org/10.48550/arXiv.2303.08774 [48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730\u201327744. [49] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy risks of generalpurpose language models. In 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 1314\u20131331. [50] Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Jirong Wen. 2020. Setrank: Learning a permutation-invariant ranking model for information retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 499\u2013508. [51] Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian Wu, Peng Jiang, Junfeng Ge, Wenwu Ou, et al. 2019. Personalized re-ranking for recommendation. In Proceedings of the 13th ACM conference on recommender systems. 3\u201311. [52] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. Measuring and Narrowing the Compositionality Gap in Language Models. arXiv:2210.03350 [cs.CL] [53] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with Language Model Prompting: A Survey. arXiv:2212.09597 [cs.CL] [54] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training user representations for improved recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4320\u20134327. [55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485\u20135551. [57] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining. IEEE, 995\u20131000. [58] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 4104\u20134113. [59] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via SelfAttentive Neural Networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 1161\u20131170. [60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [61] Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. Advances in neural information processing systems 26 (2013). [62] Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019. Multi-task feature learning for knowledge graph enhanced recommendation. In The world wide web conference. 2000\u20132010. [63] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In The world wide web conference. 3307\u20133313. [64] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023).\n[65] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD\u201917. Article 12, 7 pages. [66] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems. In Proceedings of the Web Conference 2021. 1785\u20131797. [67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022). [68] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021). [69] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023). [70] Yunjia Xi, Weiwen Liu, Jieming Zhu, Xilong Zhao, Xinyi Dai, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2022. Multi-Level Interaction Reranking with User Behavior History. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. [71] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023. Self-supervised learning for recommender systems: A survey. IEEE Transactions on Knowledge and Data Engineering (2023). [72] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A support vector method for optimizing average precision. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. 271\u2013278. [73] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. 2021. Pangu-\ud835\udefc: Largescale autoregressive pretrained Chinese language models with auto-parallel computation. arXiv preprint arXiv:2104.12369 (2021). [74] Mi Zhang, Jie Tang, Xuchen Zhang, and Xiangyang Xue. 2014. Addressing Cold Start in Recommender Systems: A Semi-Supervised Co-Training Algorithm. In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR \u201914). 73\u201382. [75] Yuhui Zhang, HAO DING, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language Models as Recommender Systems: Evaluations and Limitations. In I (Still) Can\u2019t Believe It\u2019s Not Better! NeurIPS 2021 Workshop. [76] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [77] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Leastto-Most Prompting Enables Complex Reasoning in Large Language Models. In The Eleventh International Conference on Learning Representations. [78] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for Click-through Rate Prediction (AAAI\u201919). Article 729, 8 pages. [79] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of limited access to open-world knowledge in traditional recommender systems (RSs) due to their insulated nature. It discusses previous attempts to integrate external knowledge, particularly through knowledge graphs and multi-domain learning, highlighting their limitations. The emergence of large language models (LLMs) presents an opportunity to bridge this gap by providing reasoning and factual knowledge that can enhance recommendation performance.",
        "problem": {
            "definition": "The problem this paper aims to solve is the inability of traditional recommender systems to effectively incorporate open-world knowledge, which limits their predictive accuracy and generalization capabilities.",
            "key obstacle": "The main difficulty lies in extracting and utilizing useful knowledge from LLMs, as existing methods either fail to capture user preferences accurately or struggle with the compositional gap in reasoning tasks."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of LLMs to provide rich knowledge about user preferences and item characteristics, which can be harnessed to enhance recommendation systems.",
            "opinion": "The proposed framework, KAR, integrates reasoning and factual knowledge from LLMs into traditional recommendation models, aiming to create an open-world recommender system.",
            "innovation": "KAR introduces factorization prompting to elicit accurate reasoning knowledge and employs a hybrid-expert adaptor to transform this knowledge into a format compatible with recommendation tasks, marking a significant advancement over existing methods."
        },
        "method": {
            "method name": "Open-World Knowledge Augmented Recommendation Framework",
            "method abbreviation": "KAR",
            "method definition": "KAR is a model-agnostic framework that combines classical recommender systems with open-world knowledge from LLMs, enhancing predictive accuracy by integrating reasoning and factual knowledge.",
            "method description": "KAR extracts knowledge from LLMs through a three-stage process: knowledge reasoning and generation, knowledge adaptation, and knowledge utilization.",
            "method steps": [
                "Knowledge reasoning and generation using factorization prompting to extract reasoning and factual knowledge.",
                "Knowledge adaptation to transform the generated knowledge into augmented vectors suitable for recommendation.",
                "Knowledge utilization by integrating augmented vectors into existing recommendation models."
            ],
            "principle": "The effectiveness of KAR stems from its ability to leverage both the reasoning knowledge about user preferences and the factual knowledge about items, thereby improving the overall recommendation quality."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on public datasets, including MovieLens-1M and Amazon-Books, comparing KAR's performance against various baseline methods and traditional recommendation models.",
            "evaluation method": "Performance was assessed using metrics such as AUC, LogLoss for CTR prediction, and MAP@K, NDCG@K for reranking tasks, with extensive ablation studies to analyze the impact of different components."
        },
        "conclusion": "KAR demonstrates significant improvements over state-of-the-art recommendation methods, successfully integrating open-world knowledge from LLMs and proving effective in real-world applications, as evidenced by online A/B testing results.",
        "discussion": {
            "advantage": "KAR effectively combines the strengths of LLMs and traditional recommendation algorithms, leading to enhanced performance and flexibility across various tasks.",
            "limitation": "The method may still face challenges in real-time inference due to the inherent latency of LLMs, although strategies like knowledge pre-storage help mitigate this.",
            "future work": "Future research could focus on further optimizing the integration of LLMs into recommendation systems and exploring additional sources of external knowledge."
        },
        "other info": {
            "code availability": "The code for KAR and the generated knowledge is publicly available to facilitate future research.",
            "deployment results": {
                "news platform": "7% improvement in recall metric during online A/B testing.",
                "music platform": "1.7% increase in song play count over a 7-day online A/B test."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of limited access to open-world knowledge in traditional recommender systems, highlighting the importance of recommendation algorithms in enhancing predictive accuracy and generalization capabilities."
        },
        {
            "section number": "1.2",
            "key information": "The emergence of large language models (LLMs) presents an opportunity to enhance recommendation performance by providing reasoning and factual knowledge."
        },
        {
            "section number": "2.1",
            "key information": "The problem defined in the paper is the inability of traditional recommender systems to effectively incorporate open-world knowledge, which limits their predictive accuracy."
        },
        {
            "section number": "2.3",
            "key information": "The proposed framework, KAR, integrates reasoning and factual knowledge from LLMs into traditional recommendation models, marking a significant advancement in the development of LLMs."
        },
        {
            "section number": "3.2",
            "key information": "KAR employs factorization prompting to extract reasoning knowledge and utilizes a hybrid-expert adaptor to transform this knowledge into a format compatible with recommendation tasks."
        },
        {
            "section number": "4.1",
            "key information": "KAR is a model-agnostic framework that enhances predictive accuracy by integrating reasoning and factual knowledge from LLMs into classical recommender systems."
        },
        {
            "section number": "4.2",
            "key information": "KAR's method involves knowledge reasoning and generation, knowledge adaptation, and knowledge utilization, effectively combining LLMs with traditional recommendation algorithms."
        },
        {
            "section number": "10.1",
            "key information": "The paper discusses challenges in real-time inference due to the inherent latency of LLMs, although strategies like knowledge pre-storage help mitigate this."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on optimizing the integration of LLMs into recommendation systems and exploring additional sources of external knowledge."
        },
        {
            "section number": "11",
            "key information": "KAR demonstrates significant improvements over state-of-the-art recommendation methods, proving effective in real-world applications, as evidenced by online A/B testing results."
        }
    ],
    "similarity_score": 0.7617187175062202,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/115b/115baa3e-82ce-48e1-9f27-b040bbc0cad5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef95/ef954100-87d3-4aed-aabf-bb022cd681e0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f18/3f18a628-2384-440c-ad33-b159c9a916d6.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f10e/f10e8359-f01c-4506-817b-a37b4a7f9430.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26ca/26ca453f-4e77-4177-860d-278be0da37b2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d360/d360792e-ad45-4dfe-acac-a9a59d3a07e8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c191/c191f4f3-199c-4553-97b3-63c0c9b9c1f9.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Towards open-world recommendation with knowledge augmentation from large language models.json"
}