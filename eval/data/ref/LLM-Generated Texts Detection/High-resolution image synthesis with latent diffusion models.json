{
    "from": "google",
    "scholar_id": "OmhORW5MryEJ",
    "detail_id": null,
    "title": "High-resolution image synthesis with latent diffusion models",
    "abstract": " Abstract\nAbstract\nBy decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\n# 1. Introduction\nImage synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [64,65]. In contrast, the promising results of GANs [3, 26, 3",
    "bib_name": "rombach2022high",
    "md_text": "<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/343b/343bedd7-76e4-43e4-a66e-e214aab648dc.png\" style=\"width: 50%;\"></div>\n\n# -Resolution Image Synthesis with Latent Diffusi\n# Robin Rombach1 \u2217 Andreas Blattmann1 \u2217 Dominik Lorenz1 Patrick Esser Bj\u00a8orn 1Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany Runway ML https://github.com/CompVis/latent-diffusion\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1824/182443ad-ebe1-47ae-9e08-1228169e4415.png\" style=\"width: 50%;\"></div>\n# Abstract\nAbstract\nBy decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\n# 1. Introduction\nImage synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [64,65]. In contrast, the promising results of GANs [3, 26, 39] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [79], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\u2217The first two authors contributed equally to this work.\nFigure 1. Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at 5122 px. We denote the spatial downsampling factor by f. Reconstruction FIDs [28] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8. results in image synthesis [29,82] and beyond [7,44,47,56], and define the state-of-the-art in class-conditional image synthesis [15,30] and super-resolution [70]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [82] or stroke-based synthesis [52], in contrast to other types of generative models [19,45,67]. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models [65]. Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data [16, 71]. Although the reweighted variational objective [29] aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 1000 V100 days in [15]) and repeated evaluations on a noisy version of the input space render also inference expensive,\nso that producing 50k samples takes approximately 5 days [15] on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint [63, 83]. Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]). To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility. Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis. Following common practice [11, 23, 64, 65, 93], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23,64], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs). A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [78]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [69] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3. In sum, our work makes the following contributions: (i) In contrast to purely transformer-based approaches [23,64], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff60/ff6014ec-9dea-4273-8015-e2e63819c431.png\" style=\"width: 50%;\"></div>\nFigure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [29].\napplied to high-resolution synthesis of megapixel images. (ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs. (iii) We show that, in contrast to previous work [90] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space. (iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of \u223c10242 px. (v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models. (vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github . com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [78].\n# 2. Related Work\nGenerative Models for Image Synthesis The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [26] allow for efficient sampling of high resolution images with good perceptual quality [3, 41], but are diffi-\ncult to optimize [2, 27, 53] and struggle to capture the full data distribution [54]. In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE) [45] and flow-based models [18,19] enable efficient synthesis of high resolution images [9, 43, 89], but sample quality is not on par with GANs. While autoregressive models (ARM) [6, 10, 91, 92] achieve strong performance in density estimation, computationally demanding architectures [94] and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details [16,71], maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches [23,65,97,99] use ARMs to model a compressed latent image space instead of raw pixels. Recently, Diffusion Probabilistic Models (DM) [79], have achieved state-of-the-art results in density estimation [44] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 29, 69, 82]. The best synthesis quality is usually achieved when a reweighted objective [29] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [46, 73, 81] and hierarchical approaches [30, 90], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1). Two-Stage Image Synthesis To mitigate the shortcomings of individual generative approaches, a lot of research [11,23,65,68,97,99] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [65, 97] use autoregressive models to learn an expressive prior over a discretized latent space. [64] extend this approach to textto-image generation by learning a joint distributation over discretized image and text representations. More generally, [68] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 99] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23,64], limit the overall performance of such ap-\ndata distribution [54]. In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE) [45] and flow-based models [18,19] enable efficient synthesis of high resolution images [9, 43, 89], but sample quality is not on par with GANs. While autoregressive models (ARM) [6, 10, 91, 92] achieve strong performance in density estimation, computationally demanding architectures [94] and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details [16,71], maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches [23,65,97,99] use ARMs to model a compressed latent image space instead of raw pixels. Recently, Diffusion Probabilistic Models (DM) [79], have achieved state-of-the-art results in density estimation [44] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 29, 69, 82]. The best synthesis quality is usually achieved when a reweighted objective [29] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [46, 73, 81] and hierarchical approaches [30, 90], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\nhave achieved state-of-the-art results in density estimation [44] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 29, 69, 82]. The best synthesis quality is usually achieved when a reweighted objective [29] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [46, 73, 81] and hierarchical approaches [30, 90], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n# Two-Stage Image Synthesis To mitigate the s\nings of individual generative approaches, a lot of research [11,23,65,68,97,99] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [65, 97] use autoregressive models to learn an expressive prior over a discretized latent space. [64] extend this approach to textto-image generation by learning a joint distributation over discretized image and text representations. More generally, [68] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 99] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23,64], limit the overall performance of such ap-\nproaches and less compression comes at the price of high computational cost [23,64]. Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing highfidelity reconstructions (see Fig. 1). While approaches to jointly learn an encoding/decoding model together with a score-based prior exist [90], they still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4).\n# 3. Method\nTo lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [29], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources. We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity. Such an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture [69], which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches [23, 64]. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis [25].\n# 3.1. Perceptual Image Compression\nOur perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [102] and a patch-based [32] adversarial objective [20, 23, 99]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L2 or L1 objectives. More precisely, given an image x \u2208RH\u00d7W \u00d73 in RGB space, the encoder E encodes x into a latent representation z = E(x), and the decoder D reconstructs the image from the latent, giving \u02dcx = D(z) = D(E(x)), where\nz \u2208Rh\u00d7w\u00d7c. Importantly, the encoder downsamples the image by a factor f = H/h = W/w, and we investigate different downsampling factors f = 2m, with m \u2208N. In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [45, 67], whereas VQ-reg. uses a vector quantization layer [93] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space z = E(x), we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 64], which relied on an arbitrary 1D ordering of the learned space z to model its distribution autoregressively and thereby ignored much of the inherent structure of z. Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement.\n# 3.2. Latent Diffusion Models\nDiffusion Models [79] are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T. For image synthesis, the most successful models [15,29,70] rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching [82]. These models can be interpreted as an equally weighted sequence of denoising autoencoders \u01eb\u03b8(xt, t); t = 1 . . . T, which are trained to predict a denoised variant of their input xt, where xt is a noisy version of the input x. The corresponding objective can be simplified to (Sec. A)\n(1)\nwith t uniformly sampled from {1, . . . , T}. Generative Modeling of Latent Representations With our trained perceptual compression models consisting of E and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space. Unlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space [23, 64, 99], we can take advantage of image-specific inductive biases that our model offers. This includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c21/9c218da6-b57d-4c3c-b710-7d4be45a0644.png\" style=\"width: 50%;\"></div>\nFigure 3. We condition LDMs either via concatenation or by a more general cross-attention mechanism. See Sec. 3.3 objective on the perceptually most relevant bits using the reweighted bound, which now reads\nFigure 3. We condition LDMs either via concatenation or by a more general cross-attention mechanism. See Sec. 3.3\nobjective on the perceptually most relevant bits using the reweighted bound, which now reads\n(2)\nThe neural backbone \u01eb\u03b8(\u25e6, t) of our model is realized as a time-conditional UNet [69]. Since the forward process is fixed, zt can be efficiently obtained from E during training, and samples from p(z) can be decoded to image space with a single pass through D.\n# 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [55, 80], diffusion models are in principle capable of modeling conditional distributions of the form p(z|y). This can be implemented with a conditional denoising autoencoder \u01eb\u03b8(zt, t, y) and paves the way to controlling the synthesis process through inputs y such as text [66], semantic maps [32,59] or other image-to-image translation tasks [33]. In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels [15] or blurred variants of the input image [70] is so far an under-explored area of research. We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [94], which is effective for learning attention-based models of various input modalities [34,35]. To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder \u03c4\u03b8 that projects y to an intermediate representation \u03c4\u03b8(y) \u2208RM\u00d7d\u03c4 , which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing Attention(Q, K, V ) = softmax \ufffd QKT \u221a d \ufffd \u00b7 V , with\nHere, \u03d5i(zt) \u2208RN\u00d7di \u01eb denotes a (flattened) intermediate representation of the UNet implementing \u01eb\u03b8 and W (i) V \u2208 Rd\u00d7di \u01eb, W (i) Q \u2208Rd\u00d7d\u03c4 & W (i) K \u2208Rd\u00d7d\u03c4 are learnable projection matrices [35,94]. See Fig. 3 for a visual depiction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1134/1134cf06-d2bc-4fee-8e37-9980a3a9d6fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. Samples from LDMs trained on CelebAHQ [38], FFHQ [40], LSUN-Churches [98], LSUN-Bedrooms [98] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples cf. the supplement.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6b1/b6b1582d-8119-4586-8c67-3c6ebc241da1.png\" style=\"width: 50%;\"></div>\nBased on image-conditioning pairs, we then learn the conditional LDM via\n (3)\nwhere both \u03c4\u03b8 and \u01eb\u03b8 are jointly optimized via Eq. 3. This conditioning mechanism is flexible as \u03c4\u03b8 can be parameterized with domain-specific experts, e.g. (unmasked) transformers [94] when y are text prompts (see Sec. 4.3.1)\n# 4. Experiments\nLDMs provide means to flexible and computationally tractable diffusion based image synthesis also including high-resolution generation of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixelbased diffusion models in both training and inference. Interestingly, we find that LDMs trained in VQ-regularized latent spaces achieve better sample quality, even though the reconstruction capabilities of VQ-regularized first stage models slightly fall behind those of their continuous counterparts, cf. Tab. 8. Therefore, we evaluate VQ-regularized LDMs in the remainder of the paper, unless stated differently. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions higher than 2562 can be found in Appendix C.1. In D.2 we furthermore list details on architecture, implementation, training and evaluation for all results presented in this section.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8b9/c8b9f04c-bb8e-45dc-947f-8bdb71c03886.png\" style=\"width: 50%;\"></div>\n# 4.1. On Perceptual Compression Tradeoffs\nThis section analyzes the behavior of our LDMs with different downsampling factors f \u2208{1, 2, 4, 8, 16, 32} (abbreviated as LDM-f, where LDM-1 corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters. Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs compared in this section. Fig. 5 shows sample quality as a function of training progress for 2M steps of class-conditional\nFigure 5. Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the ImageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [81] and \u03ba = 0.\nFigure 6. Inference speed vs sample quality: Comparing LDMs with different amounts of compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate {10, 20, 50, 100, 200} sampling steps with the DDIM sampler, counted from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM-{4-8} compared to models with different compression ratios. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100. models on the ImageNet [12] dataset. We see that, i) small downsampling factors for LDM-{1,2} result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. LDM-{4-16} strike a good balance between efficiency and perceptually faithful results, which manifests in a sig-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a56a/a56acb7a-5288-4dee-a871-f085bf408ba5.png\" style=\"width: 50%;\"></div>\nCelebA-HQ 256 \u00d7 256\nFFHQ 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nDC-VAE [61]\n15.8\n-\n-\nImageBART [21]\n9.57\n-\n-\nVQGAN+T. [23] (k=400)\n10.2\n-\n-\nU-Net GAN (+aug) [75]\n10.9 (7.6)\n-\n-\nPGGAN [38]\n8.0\n-\n-\nUDM [42]\n5.54\n-\n-\nLSGM [90]\n7.22\n-\n-\nStyleGAN [40]\n4.16\n0.71\n0.46\nUDM [42]\n7.16\n-\n-\nProjectedGAN [74]\n3.08\n0.65\n0.46\nLDM-4 (ours, 500-s\u2020)\n5.11\n0.72\n0.49\nLDM-4 (ours, 200-s)\n4.98\n0.73\n0.50\nLSUN-Churches 256 \u00d7 256\nLSUN-Bedrooms 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nDDPM [29]\n7.89\n-\n-\nImageBART [21]\n5.51\n-\n-\nImageBART [21]\n7.32\n-\n-\nDDPM [29]\n4.9\n-\n-\nPGGAN [38]\n6.42\n-\n-\nUDM [42]\n4.57\n-\n-\nStyleGAN [40]\n4.21\n-\n-\nStyleGAN [40]\n2.35\n0.59\n0.48\nStyleGAN2 [41]\n3.86\n-\n-\nADM [15]\n1.90\n0.66\n0.51\nProjectedGAN [74]\n1.59\n0.61\n0.44\nProjectedGAN [74]\n1.52\n0.61\n0.34\nLDM-8\u2217(ours, 200-s)\n4.02\n0.64\n0.52\nLDM-4 (ours, 200-s)\n2.95\n0.66\n0.48\nTable 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [42,61,96], FFHQ from [41, 42]. \u2020: N-s refers to N sampling steps with the DDIM [81] sampler. \u2217: trained in KL-regularized latent space. Additional results can be found in the supplementary.\nnificant FID [28] gap of 38 between pixel-based diffusion (LDM-1) and LDM-8 after 2M training steps. In Fig. 6, we compare models trained on CelebAHQ [38] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [81] and plot it against FID-scores [28]. LDM-{4-8} outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based LDM-1, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. Summarized, we observe that LDM-4 and -8 lie in the best behaved regime for achieving high-quality synthesis results.\nText-Conditional Image Synthesis\nDALL-E\u2020 [64]\nCogView\u2020 [17]\nLafite\u2020 [105]\nLDM-KL-8\nLDM-KL-8-G\u2217\nFID \u2193\n27.50\n27.10\n26.94\n23.35\n12.61\nIS \u2191\n17.90\n18.20\n26.02\n19.93\u00b10.35\n26.62\u00b10.38\nTable 2. Evaluation of text-conditional image synthesis on the MS-COCO [50] dataset: Our model outperforms autoregressive [17, 64] and GAN-based [105] methods by a significant margin when using 250 DDIM [81] steps .\u2020: Numbers taken from [105]. \u2217: Classifier-free guidance [31], scale 1.5.\n# 4.2. Image Generation with Latent Diffusion  \nWe train unconditional models of 2562 images on CelebA-HQ [38], FFHQ [40], LSUN-Churches and -Bedrooms [98] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [28] and ii) Precision-and-Recall [49]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [90] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. 1-2. We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close\n<div style=\"text-align: center;\">Figure 7. Top: Samples from our LDM for layout-to-image synthesis on COCO [4]. Quantitative evaluation in the supplement. Bottom: Samples from our text-to-image LDM model for userdefined text prompts, which is trained on LAION-400M [76].</div>\nto ADM [15], despite utilizing half its parameters and requiring 4-times less train resources (see Appendix D.3.5). Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset.\n# 4.3. Conditional Latent Diffusion\n# 4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For text-toimage image modeling, we train a 1.45B parameter model conditioned on language prompts on LAION-400M [76]. We employ the BERT-tokenizer [14] and implement \u03c4\u03b8 as a transformer [94] to infer a latent code which is mapped into the UNet via cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf. Fig. 7 and 14. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [50] validation set, where our model improves upon powerful AR [17,64] and GAN-based [105] methods, cf. Tab. 2. We note that applying classifier-free diffusion guidance [31] greatly boosts sample quality. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages [48], and finetune on COCO [4], see Fig. 7. See Sec. C.4 for the quantitative evaluation and implementation details. Lastly, following prior work [3, 15, 21, 23], we evaluate our best-performing class-conditional ImageNet mod-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/062b/062b5fd9-16e4-486c-b25a-a418668dd745.png\" style=\"width: 50%;\"></div>\nels with f \u2208{4, 8} from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. C.5. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, cf. Tab 18.\nMethod\nFID\u2193\nIS\u2191\nPrecision\u2191\nRecall\u2191\nNparams\nBigGan-deep [3]\n6.95\n203.6\u00b12.6\n0.87\n0.28\n340M\n-\nADM [15]\n10.94\n100.98\n0.69\n0.63\n554M\n250 DDIM steps\nADM-G [15]\n4.59\n186.7\n0.82\n0.52\n608M\n250 DDIM steps\nLDM-4 (ours)\n10.56\n103.49\u00b11.24\n0.71\n0.62\n400M\n250 DDIM steps\nLDM-4-G (ours)\n3.60\n247.67\u00b15.59\n0.87\n0.48\n400M\n250 steps, classifier-free guidance [31], scale 1.5\nTable 3. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet [12] dataset. A more detailed comparison with additional baselines can be found in C.5, Tab. 10 and E.\n# 3.2 Convolutional Sampling Beyond 2562\nBy concatenating spatially aligned conditioning information to the input of \u01eb\u03b8, LDMs can serve as efficient generalpurpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps [23, 59] and concatenate downsampled versions of the semantic maps with the latent image representation of a f = 4 model (VQ-reg., see Tab. 8). We train on an input resolution of 2562 (crops from 3842) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see Fig. 8). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between 5122 and 10242. For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. C.1 we illustrate this when learning an LDM on (i) the latent space as provided by a f = 4 model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b40e/b40eeac9-2b40-4e96-8731-fb2fa287a4d1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8. A LDM trained on 2562 resolution can generalize to larger resolution (here: 512\u00d71024) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.</div>\nLDMs can be efficiently trained for super-resolution by diretly conditioning on low-resolution images via concate-\n<div style=\"text-align: center;\">Figure 9. ImageNet 64\u2192256 super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [70].</div>\nnation (cf. Sec. 3.3). In a first experiment, we follow SR3 [70] and fix the image degradation to a bicubic interpolation with 4\u00d7-downsampling and train on ImageNet following SR3\u2019s data processing pipeline. We use the f = 4 autoencoding model pretrained on OpenImages (VQ-reg., cf. Tab. 8) and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e. \u03c4\u03b8 is the identity. Our qualitative and quantitative results (see Fig. 9 and Tab. 4) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception [102] and favor blurriness over imperfectly aligned high frequency details [70]. Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 [70] where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 5 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a posthoc guiding mechanism [15] and we implement this imagebased guider via a perceptual loss, see Sec. C.7. Since the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. C.7.1.\n<div style=\"text-align: center;\">ation. The results are shown in Sec. C.7.1.</div>\ndation. The results are shown in Sec. C.7.1.\nMethod\nFID \u2193\nIS \u2191\nPSNR \u2191\nSSIM \u2191\nNparams\nThroughput\u2217[ samples\ns\n]\nImage Regression [70]\n15.2\n121.1\n27.9\n0.801\n625M\nN/A\nSR3 [70]\n5.2\n180.1\n26.4\n0.762\n625M\nN/A\nLDM-4 (ours, 100 steps)\n2.8\u2020/4.8\u2021\n166.3\n24.4\u00b13.8\n0.69\u00b10.14\n169M\n4.62\nLDM-4 (ours, big, 100 steps)\n2.4\u2020/4.3\u2021\n174.9\n24.7\u00b14.1\n0.71\u00b10.15\n552M\n4.5\nLDM-4 (ours, 50 steps, guiding)\n4.4\u2020/6.4\u2021\n153.7\n25.8\u00b13.7\n0.74\u00b10.12\n184M\n0.38\nTable 4. \u00d74 upscaling results on ImageNet-Val. (2562); \u2020: FID features computed on validation split, \u2021: FID features computed on train split; \u2217: Assessed on a NVIDIA A100\nTable 4. \u00d74 upscaling results on ImageNet-Val. (2562); \u2020: FID features computed on validation split, \u2021: FID features computed on train split; \u2217: Assessed on a NVIDIA A100\n# 4.5. Inpainting with Latent Diffusion\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are\nSR on ImageNet\nInpainting on Places\nUser Study\nPixel-DM (f1)\nLDM-4\nLAMA [85]\nLDM-4\nTask 1: Preference vs GT \u2191\n16.0%\n30.4%\n13.6%\n21.0%\nTask 2: Preference Score \u2191\n29.4%\n70.6%\n31.9%\n68.1%\n40-50% masked\nAll samples\nMethod\nFID \u2193\nLPIPS \u2193\nFID \u2193\nLPIPS \u2193\nLDM-4 (ours, big, w/ ft)\n9.39\n0.246\u00b1 0.042\n1.50\n0.137\u00b1 0.080\nLDM-4 (ours, big, w/o ft)\n12.89\n0.257\u00b1 0.047\n2.40\n0.142\u00b1 0.085\nLDM-4 (ours, w/ attn)\n11.87\n0.257\u00b1 0.042\n2.15\n0.144\u00b1 0.084\nLDM-4 (ours, w/o attn)\n12.60\n0.259\u00b1 0.041\n2.37\n0.145\u00b1 0.084\nLaMa [85]\u2020\n12.31\n0.243\u00b1 0.038\n2.23\n0.134\u00b1 0.080\nLaMa [85]\n12.0\n0.24\n2.21\n0.14\nCoModGAN [103]\n10.4\n0.26\n1.82\n0.15\nRegionWise [51]\n21.3\n0.27\n4.75\n0.15\nDeepFill v2 [100]\n22.1\n0.28\n5.20\n0.16\nEdgeConnect [57]\n30.5\n0.28\n8.37\n0.16\nTable 5. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in D.3.6\ntrain throughput\nsampling throughput\u2020\ntrain+val\nFID@2k\nModel (reg.-type)\nsamples/sec.\n@256\n@512\nhours/epoch\nepoch 6\nLDM-1 (no first stage)\n0.11\n0.26\n0.07\n20.66\n24.74\nLDM-4 (KL, w/ attn)\n0.32\n0.97\n0.34\n7.66\n15.21\nLDM-4 (VQ, w/ attn)\n0.33\n0.97\n0.34\n7.04\n14.99\nLDM-4 (VQ, w/o attn)\n0.35\n0.99\n0.36\n6.66\n15.95\nTable 6. Assessing inpainting efficiency. \u2020: Deviations from Fig. 6 due to varying GPU settings/batch sizes cf. the supplement.\nare corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [85], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. We describe the exact training & evaluation protocol on Places [104] in Sec. D.2.2. We first analyze the effect of different design choices for the first stage. We compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution 2562 and 5122, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7\u00d7 between pixel- and latent-based diffusion models while improving FID scores by a factor of at least 1.6\u00d7. The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of [85]. LPIPS between the unmasked images and our samples is slightly higher than that of [85]. We attribute this to [85] only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf. Fig. 20. Additionally in a user study (Tab. 5) human subjects favor our results over those of [85]. Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention. Following [15], the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN [3] residual block for up- and downsampling and has 387M parameters instead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions 2562 and 5122, which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half\n<div style=\"text-align: center;\">40-50% masked</div>\nTable 7. Comparison of inpainting performance on 30k crops of size 512 \u00d7 512 from test images of Places [104]. The column 4050% reports metrics computed over hard examples where 40-50% of the image region have to be inpainted. \u2020recomputed on our test set, since the original test set used in [85] was not available.\nan epoch at resolution 5122 allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ ft in Tab. 7, Fig. 10.).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7518/75189eb4-3407-4af4-a884-3ecec90df14f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10. Qualitative results on object removal with our big, w/ ft inpainting model. For more results, see Fig. 21.</div>\n# 5. Conclusion\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.\nThis work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project KI-Absicherung - Safe AI for automated driving and by the German Research Foundation (DFG) project 421703927.\n[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122\u20131131. IEEE Computer Society, 2017. 1 [2] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein gan, 2017. 3 [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019. 1, 2, 6, 7, 8, 19, 26 [4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 1822, 2018, pages 1209\u20131218. Computer Vision Foundation / IEEE Computer Society, 2018. 6, 17, 18 [5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021. 27 [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 1691\u20131703. PMLR, 2020. 3 [7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR. OpenReview.net, 2021. 1 [8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. In NeurIPS, 2020. 8 [9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020. 3 10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. 3 11] Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net, 2019. 2, 3 12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255. IEEE Computer Society, 2009. 1, 5, 7, 19 13] Emily Denton. Ethical considerations of generative ai. AI for Content Creation Workshop, CVPR, 2021. 27 14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. 6 15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021. 1, 2, 3, 4, 6, 7, 8, 15, 19, 23, 24, 26\n[16] Sander Dieleman. Musings on typicality, 2020. 1, 3 [17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-toimage generation via transformers. CoRR, abs/2105.13290, 2021. 6 [18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation, 2015. 3 [19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 1, 3 [20] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Adv. Neural Inform. Process. Syst., pages 658\u2013666, 2016. 3 [21] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bj\u00a8orn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. CoRR, abs/2108.08827, 2021. 6, 19 [22] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. A note on data biases in generative models. arXiv preprint arXiv:2012.02516, 2020. 27 [23] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming transformers for high-resolution image synthesis. CoRR, abs/2012.09841, 2020. 2, 3, 4, 6, 7, 18, 19, 27, 32, 34 [24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free speech delusions. Md. L. Rev., 78:892, 2018. 27 [25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through languageimage encoders. ArXiv, abs/2106.14843, 2021. 3 [26] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014. 1, 2 [27] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017. 3 [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., pages 6626\u2013 6637, 2017. 1, 6, 24 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 4, 6, 14 [30] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021. 1, 3, 19 [31] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 6, 7, 19, 26, 35, 36\n[32] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 5967\u20135976. IEEE Computer Society, 2017. 3, 4 [33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967\u20135976, 2017. 4 [34] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. H\u00b4enaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver IO: A general architecture for structured inputs &outputs. CoRR, abs/2107.14795, 2021. 4 [35] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Jo\u02dcao Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4651\u20134664. PMLR, 2021. 4 [36] Manuel Jahn, Robin Rombach, and Bj\u00a8orn Ommer. Highresolution complex scene synthesis with transformers. CoRR, abs/2105.06458, 2021. 17, 18, 25 [37] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses. arXiv preprint arXiv:2001.09528, 2020. 27 [38] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 5, 6 [39] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401\u2013 4410, 2019. 1 [40] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 5, 6 [41] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. CoRR, abs/1912.04958, 2019. 2, 6, 26 [42] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching model for unbounded data score. CoRR, abs/2106.05527, 2021. 6 [43] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, 2018. 3 [44] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021. 1, 3, 14\n[45] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR, 2014. 1, 3, 4, 27 [46] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. CoRR, abs/2106.00132, 2021. 3 [47] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR. OpenReview.net, 2021. 1 [48] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018. 6, 17, 18 [49] Tuomas Kynk\u00a8a\u00a8anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019. 6, 24 [50] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 6, 25 [51] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan Liu, Dacheng Tao, and Edwin Hancock. Region-wise generative adversarial imageinpainting for large missing areas. ArXiv, abs/1909.12507, 2019. 8 [52] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. CoRR, abs/2108.01073, 2021. 1 [53] Lars M. Mescheder. On the convergence properties of GAN training. CoRR, abs/1801.04406, 2018. 3 [54] Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. Unrolled generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 3 [55] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014. 4 [56] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. CoRR, abs/2103.16091, 2021. 1 [57] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. ArXiv, abs/1901.00212, 2019. 8 [58] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. 24, 25 [59] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 4, 7\n[60] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 18 [61] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 823\u2013832. Computer Vision Foundation / IEEE, 2021. 6 [62] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021. 24 [63] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021. 2 [64] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. 1, 2, 3, 4, 6, 18, 25 [65] Ali Razavi, A\u00a8aron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 14837\u201314847, 2019. 1, 2, 3, 19 [66] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016. 4 [67] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014. 1, 4, 27 [68] Robin Rombach, Patrick Esser, and Bj\u00a8orn Ommer. Network-to-network translation with conditional invertible neural networks. In NeurIPS, 2020. 3 [69] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234\u2013241. Springer, 2015. 2, 3, 4 [70] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. CoRR, abs/2104.07636, 2021. 1, 4, 7, 19, 20, 21, 25 [71] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017. 1, 3 [72] Dave Salvator. NVIDIA Developer Blog. https: / / developer . nvidia . com / blog / getting immediate-speedups-with-a100-tf32, 2020. 26 [73] Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. CoRR, abs/2104.02600, 2021. 3 [74] Axel Sauer, Kashyap Chitta, Jens M\u00a8uller, and Andreas Geiger. Projected gans converge faster. CoRR, abs/2111.01007, 2021. 6\n[75] Edgar Sch\u00a8onfeld, Bernt Schiele, and Anna Khoreva. A unet based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204\u20138213. Computer Vision Foundation / IEEE, 2020. 6 [76] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion400m: Open dataset of clip-filtered 400 million image-text pairs, 2021. 6 [77] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn. Represent., 2015. 27, 41, 42, 43 [78] Charlie Snell. Alien Dreams: An Emerging Art Scene. https : / / ml . berkeley . edu / blog / posts / clip-art/, 2021. [Online; accessed November-2021]. 2 [79] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015. 1, 3, 4, 15 [80] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. 4 [81] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net, 2021. 3, 5, 6, 20 [82] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020. 1, 3, 4, 15 [83] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13693\u201313696. AAAI Press, 2020. 2 [84] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs/2003.11571, 2020. 18, 25 [85] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. ArXiv, abs/2109.07161, 2021. 8, 24, 30 [86] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Thirty-Fifth AAAI Conference on\nArtificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 2647\u20132655. AAAI Press, 2021. 17, 18, 25 [87] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist... but it might be yours! identity leakage in generative models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1320\u20131328, 2021. 27 [88] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521\u20131528. IEEE, 2011. 27 [89] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020. 3 [90] Arash Vahdat, Karsten Kreis, and Jan Kautz. Scorebased generative modeling in latent space. CoRR, abs/2106.05931, 2021. 2, 3, 6 [91] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016. 3 [92] A\u00a8aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016. 3 [93] A\u00a8aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pages 6306\u20136315, 2017. 2, 4, 27 [94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998\u20136008, 2017. 3, 4, 5, 6 [95] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface\u2019s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. 24 [96] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 6 [97] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021. 3 [98] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 5, 6 [99] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2021. 3, 4 100] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with\ngated convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4470\u20134479, 2019. 8 [101] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. ArXiv, abs/2103.14006, 2021. 21 [102] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 3, 7, 16 [103] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I-Chao Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. ArXiv, abs/2103.10428, 2021. 8 [104] Bolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452\u20131464, 2018. 8, 24 [105] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: towards language-free training for text-to-image generation. CoRR, abs/2111.13792, 2021. 6\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of high-resolution image synthesis, which has significant computational demands. Previous methods, particularly GANs, have limited variability and struggle with complex distributions. Diffusion models (DMs) have shown impressive results but are computationally intensive, requiring hundreds of GPU days for training. To democratize access to high-resolution synthesis, the authors propose a new method that reduces computational requirements without sacrificing quality.",
        "problem": {
            "definition": "The problem is the high computational cost and inefficiency of training and evaluating diffusion models for high-resolution image synthesis, which limits accessibility.",
            "key obstacle": "The core obstacle is the excessive computational resources required for training and inference, which is exacerbated by the high-dimensional pixel space in which traditional DMs operate."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is to leverage the latent space of pretrained autoencoders to reduce the computational complexity of diffusion models while maintaining high visual fidelity.",
            "opinion": "The proposed idea involves training diffusion models in a latent space, allowing for efficient high-resolution image synthesis without the need for heavy spatial downsampling.",
            "innovation": "The innovation lies in the introduction of latent diffusion models (LDMs) that operate in a lower-dimensional latent space, significantly improving efficiency and enabling high-resolution synthesis without compromising detail."
        },
        "method": {
            "method name": "Latent Diffusion Models",
            "method abbreviation": "LDM",
            "method definition": "LDMs are diffusion models trained in the latent space of pretrained autoencoders, allowing for efficient high-resolution image synthesis.",
            "method description": "LDMs utilize a two-stage approach where an autoencoder compresses images into a latent space, and a diffusion model generates images from this latent representation.",
            "method steps": [
                "Train an autoencoder to learn a perceptually equivalent latent space.",
                "Use the latent representation to train a diffusion model that generates images from the compressed data."
            ],
            "principle": "LDMs are effective because they exploit the inductive biases of diffusion models while operating in a lower-dimensional space, allowing for efficient training and inference."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using various datasets including CelebA-HQ, FFHQ, and ImageNet. The evaluation compared LDMs against pixel-based diffusion models, assessing performance metrics such as FID and sample quality.",
            "evaluation method": "The performance was measured through metrics like FID, precision, and recall, with comparisons made across different downsampling factors and model configurations."
        },
        "conclusion": "The study demonstrates that latent diffusion models significantly enhance the training and sampling efficiency of diffusion models while maintaining high-quality outputs across various image synthesis tasks.",
        "discussion": {
            "advantage": "The key advantages of LDMs include reduced computational costs, improved training efficiency, and the ability to generate high-resolution images without excessive resource requirements.",
            "limitation": "One limitation is that while LDMs improve efficiency, they may still face challenges in specific tasks that require highly specialized architectures.",
            "future work": "Future research could explore further optimizations of the latent space and the application of LDMs to additional generative tasks beyond image synthesis."
        },
        "other info": {
            "additional details": [
                {
                    "info1": "The authors released pretrained latent diffusion and autoencoding models on GitHub for reusable applications."
                },
                {
                    "info2": {
                        "info2.1": "The method facilitates various conditioning mechanisms, including text-to-image and image-to-image synthesis."
                    }
                }
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "5.1",
            "key information": "The proposed method, Latent Diffusion Models (LDMs), allows for efficient high-resolution image synthesis across various media."
        },
        {
            "section number": "5.2",
            "key information": "Latent diffusion models significantly enhance training and sampling efficiency while maintaining high-quality outputs in image synthesis tasks."
        },
        {
            "section number": "7.1",
            "key information": "The key advantages of LDMs include reduced computational costs and improved training efficiency, addressing the challenge of high computational demands in image synthesis."
        },
        {
            "section number": "7.4",
            "key information": "Future research could explore further optimizations of the latent space and the application of LDMs to additional generative tasks beyond image synthesis."
        },
        {
            "section number": "6.1",
            "key information": "LDMs facilitate various conditioning mechanisms, including text-to-image and image-to-image synthesis, demonstrating synergies between large language models and AI-generated content."
        }
    ],
    "similarity_score": 0.5826527098341038,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/343b/343bedd7-76e4-43e4-a66e-e214aab648dc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1824/182443ad-ebe1-47ae-9e08-1228169e4415.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff60/ff6014ec-9dea-4273-8015-e2e63819c431.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c21/9c218da6-b57d-4c3c-b710-7d4be45a0644.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1134/1134cf06-d2bc-4fee-8e37-9980a3a9d6fa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6b1/b6b1582d-8119-4586-8c67-3c6ebc241da1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8b9/c8b9f04c-bb8e-45dc-947f-8bdb71c03886.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a56a/a56acb7a-5288-4dee-a871-f085bf408ba5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/062b/062b5fd9-16e4-486c-b25a-a418668dd745.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b40e/b40eeac9-2b40-4e96-8731-fb2fa287a4d1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7518/75189eb4-3407-4af4-a884-3ecec90df14f.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0011_large/papers/High-resolution image synthesis with latent diffusion models.json"
}